# ####ä¸€ã€å¤„ç†æ•°æ®
# import json
# import os
#
# # ğŸ“‚ å®šä¹‰æ–‡ä»¶è·¯å¾„
# input_file = "/data/zhenmengyuan/LLaMA-Factory/data/ko_data/eval_NER_en2ko.json"
# output_dir = "/data/zhenmengyuan/LLaMA-Factory/saves/task_eval/ko-en/NER/"
# output_file = os.path.join(output_dir, "task_NER.json")
# demo_output_file = os.path.join(output_dir, "task_NER_demo.json")
#
# # ğŸ“‚ ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
# os.makedirs(output_dir, exist_ok=True)
#
# # âœ… è¯»å– eval_NER_en2ko.json æ–‡ä»¶
# with open(input_file, "r", encoding="utf-8") as f:
#     eval_data = json.load(f)
#
# # âœ… éœ€è¦è¿‡æ»¤çš„å®ä½“ç±»åˆ«åˆ—è¡¨ï¼ˆæ³¨æ„AFWå¯èƒ½ç¬”è¯¯ä¸ºAFMï¼Œå·²åŒ…å«AFWï¼‰
# FILTER_ENTITIES = {"ANM", "MAT", "FLD", "PLT", "TRM", "AFW", "CVL"}  # AFMæ”¹ä¸ºAFW
#
#
# def filter_entities(output_str):
#     """è¿‡æ»¤ä¸éœ€è¦çš„å®ä½“ç±»åˆ«"""
#     filtered = []
#     entities = [e.strip() for e in output_str.split(",")]
#
#     for entity in entities:
#         if ":" in entity:
#             # åˆ†å‰²å®ä½“ç±»å‹å’Œå†…å®¹
#             entity_type, content = entity.split(":", 1)
#             entity_type = entity_type.strip()
#
#             # ä¿ç•™ä¸åœ¨è¿‡æ»¤åˆ—è¡¨ä¸­çš„å®ä½“
#             if entity_type not in FILTER_ENTITIES:
#                 # é‡æ„æ ‡å‡†åŒ–æ ¼å¼
#                 filtered.append(f"{entity_type}: {content.strip()}")
#
#     return ", ".join(filtered)
#
#
# # âœ… è½¬æ¢æ ¼å¼
# task_data = []
# for item in eval_data:
#     original_output = item.get("output", "")
#     cleaned_output = filter_entities(original_output)
#
#     task_data.append({
#         "instruction": "Please complete the named entity recognition task. You only need to output PER (person name), LOC (place name), ORG (organization name), DAT (date), TIM (time), NUM (number/quantity), and EVT (event) in the following text. The output format is as follows: PER: ,LOC: ,ORG: ... You only need to output the entity that exist, and do not output those that do not exist.",
#         "input": item["history"][0][1],
#         "output": cleaned_output
#     })
#
# demo_data = task_data[:10]
#
# # âœ… ä¿å­˜æ–‡ä»¶
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(task_data, f, ensure_ascii=False, indent=4)
#
# with open(demo_output_file, "w", encoding="utf-8") as f:
#     json.dump(demo_data, f, ensure_ascii=False, indent=4)
#
# print(f"è½¬æ¢å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜è‡³: {output_file}")

#
import json

# åŸå§‹æ–‡ä»¶è·¯å¾„
input_file = "task_LCTW_NER.json"
#output_file = "task_nohistory_NER_demo.json"

# # æ–°çš„instructionå†…å®¹
new_instruction = "Please complete the NER task. You only need to output PER (person name), LOC (place name), ORG (organization name), DAT (full date, such as in the first quarter, 6 years, this coming November), TIM (time, such as morning, 90 minutes, at 1:50 AM), NUM (full entity including the number, such as No. 1517, 25 wins and 1 loss, 10 ladles, 13 horses, step 30), and EVT (full event, such as the second round of the professional basketball playoffs, â—U-18 High School Club League (Incheon)) in the following text. The output format is as follows: PER: ,LOC: ,ORG: ... Only output the entity that exist, and do not output those that do not exist."

# è¯»å–åŸå§‹æ•°æ®
with open(input_file, "r", encoding="utf-8") as f:
    data = json.load(f)

# ä¿®æ”¹å‰50æ¡æ•°æ®çš„instructionå­—æ®µ
for item in data:
    item["instruction"] = new_instruction

# ä¿å­˜ä¿®æ”¹åçš„å®Œæ•´æ–‡ä»¶
with open(input_file, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)
# # åªä¿ç•™å‰50æ¡æ•°æ®
# demo_data = data[:50]
#
# # ä¿å­˜ä¿®æ”¹åçš„æ•°æ®
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(demo_data, f, ensure_ascii=False, indent=4)

print(f"1. å·²æ›´æ–°åŸå§‹æ–‡ä»¶ {input_file} çš„æ‰€æœ‰instructionå­—æ®µ")
#print(f"2. å·²ç”ŸæˆåŒ…å«å‰50æ¡æ•°æ®çš„demoæ–‡ä»¶ {output_file}")


#####æ¨¡å‹è¯„ä¼°
# import os
# import json
# import openai
# import time
# import unicodedata
# import re
# import torch
# from tqdm import tqdm
# from rouge import Rouge
# from collections import Counter
# from openai import OpenAI # å¯¼å…¥OpenAI
# from sacrebleu.metrics import BLEU
# import botok
# from botok import WordTokenizer
# from botok.config import Config
# from pathlib import Path
#
# # æ–‡ä»¶è·¯å¾„
# data_file = "task_NER.json"
# output_results_file = "gpt-4o_outputs.json"
# eval_results_file = "gpt-4o_results.json"
#
# # åŠ è½½æ•°æ®é›†
# with open(data_file, "r", encoding="utf-8") as f:
#     dataset = json.load(f)
#
# # è¯»å–å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
# if os.path.exists(output_results_file):
#     with open(output_results_file, "r", encoding="utf-8") as f:
#         existing_results = json.load(f)
# else:
#     existing_results = []
#
# # **å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„æ•°æ®**
# processed_inputs = {item["input"] for item in existing_results}
# dataset = [item for item in dataset if item["input"] not in processed_inputs]
#
# # âœ… OpenAI API å®¢æˆ·ç«¯ï¼ˆé€‚é…æ–°ç‰ˆ OpenAI SDKï¼‰
# client = OpenAI(base_url = "http://chatapi.littlewheat.com/v1",
#                 api_key  = "sk-p4oPXDT8fchQaVROS85cRlsGy9vd8zq6511QfSSgzUVUiBPo")
#
# # **OpenAI API è°ƒç”¨**
# def get_gpt4o_response(instruction, input_text, max_retries=3):
#     for attempt in range(max_retries):
#         messages = [
#             {"role": "system", "content": "You are an assistant who is good at NER task, no need to explain anything."},
#             {"role": "user", "content": f"{instruction}\n\n{input_text}"}
#         ]
#         try:
#             response = client.chat.completions.create(
#                 model="gpt-4o",
#                 messages=messages,
#                 temperature=0.8,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
#             )
#             return response.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
#             if attempt == max_retries - 1:
#                 return "ERROR"
#             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
#
# def extract_entities(text):
#     """æ”¹è¿›ç‰ˆå®ä½“æå–å‡½æ•°ï¼Œæ­£ç¡®å¤„ç†ç©ºæ ¼åˆ†éš”çš„å®ä½“ç±»å‹"""
#     entities = {'PER': set(), 'LOC': set(), 'ORG': set(),'DAT': set(), 'TIM': set(), 'NUM': set(), 'EVT': set()}
#
#     # æ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ•è·å®ä½“å†…å®¹ç›´åˆ°ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–å­—ç¬¦ä¸²æœ«å°¾
#     pattern = r"""
#         (?i)                    # å¿½ç•¥å¤§å°å†™
#         \b(PER|LOC|ORG|DAT|TIM|NUM|EVT)\b       # å®ä½“ç±»å‹ä½œä¸ºç‹¬ç«‹å•è¯
#         \s*:\s*                 # å†’å·å‰åå¯èƒ½æœ‰ç©ºæ ¼
#         (                       # æ•è·å®ä½“å†…å®¹
#             (?:                 # éæ•è·ç»„ï¼Œç¡®ä¿ä¸è·¨å®ä½“ç±»å‹
#                 (?!\b(?:PER|LOC|ORG|DAT|TIM|NUM|EVT)\b\s*:)  # è´Ÿå‘å‰ç»ï¼Œæ’é™¤ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹
#                 .               # åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
#             )*?                 # éè´ªå©ªåŒ¹é…ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–æœ«å°¾
#         )
#         (?=\s*\b(?:PER|LOC|ORG|DAT|TIM|NUM|EVT)\b\s*:|$|\s*$)  # æ­£å‘æ–­è¨€ï¼Œç¡®ä¿åœåœ¨ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–å­—ç¬¦ä¸²æœ«å°¾
#     """
#
#     matches = re.findall(pattern, text, re.VERBOSE)
#
#     for ent_type, ent_text in matches:
#         ent_type = ent_type.upper()
#         if not ent_text or ent_text.strip().lower() in ['none']:
#             continue
#
#         # æ¸…æ´—å†…å®¹ï¼šå»å¼•å·ã€å½’ä¸€åŒ–ã€å»ç©ºæ ¼
#         cleaned = ent_text.strip().replace("'", "").replace('"', '')
#         cleaned = unicodedata.normalize('NFC', cleaned)
#         cleaned = re.sub(r'\s+', '', cleaned)  # å»é™¤æ‰€æœ‰ç©ºæ ¼
#
#         # ä½¿ç”¨è—æ–‡é€—å·å’Œä¸­è‹±æ–‡é€—å·åˆ†å‰²
#         parts = [p for p in re.split(r'[,\u0f0d]', cleaned) if p]
#
#         # è¿‡æ»¤æ‰å¯èƒ½è¯¯åŒ¹é…çš„å®ä½“ç±»å‹å…³é”®è¯
#         filtered_parts = []
#         for part in parts:
#             if part.strip().upper() not in {'PER', 'LOC', 'ORG', 'DAT', 'TIM', 'NUM', 'EVT'}:
#                 filtered_parts.append(part)
#
#         if ent_type in entities and filtered_parts:
#             entities[ent_type].update(filtered_parts)
#
#     return entities
#
#
# def compute_entity_f1(gold_entities, pred_entities):
#     """
#     æ”¹è¿›ç‰ˆF1è®¡ç®—ï¼Œç²¾ç¡®å¤„ç†ç©ºå€¼æƒ…å†µï¼š
#     1. å½“ä¸”ä»…å½“é»„é‡‘æ ‡å‡†æˆ–é¢„æµ‹æ ‡å‡†ä¸­æŸç±»å‹å­˜åœ¨å®ä½“æ—¶æ‰è®¡ç®—è¯¥ç±»å‹
#     2. å½“ä¸¤è€…éƒ½ä¸ºç©ºæ—¶ï¼Œè¯¥ç±»å‹ä¸å‚ä¸è®¡ç®—
#     3. å®å¹³å‡åªè®¡ç®—å®é™…å‚ä¸çš„ç±»å‹
#     """
#     f1_scores = []
#     entity_types = ['PER', 'LOC', 'ORG', 'DAT', 'TIM', 'NUM', 'EVT']
#
#     for ent_type in entity_types:
#         gold_set = gold_entities.get(ent_type, set())
#         pred_set = pred_entities.get(ent_type, set())
#
#         # åˆ¤æ–­æ˜¯å¦éœ€è¦è®¡ç®—è¯¥ç±»å‹
#         gold_has_entities = len(gold_set) > 0
#         pred_has_entities = len(pred_set) > 0
#
#         if not gold_has_entities and not pred_has_entities:
#             continue  # åŒæ–¹éƒ½ä¸ºç©ºæ—¶ä¸å‚ä¸è®¡ç®—
#
#         tp = len(gold_set & pred_set)
#         fp = len(pred_set - gold_set)
#         fn = len(gold_set - pred_set)
#
#         # å¤„ç†åˆ†æ¯ä¸ºé›¶çš„æƒ…å†µ
#         precision = tp / (tp + fp) if (tp + fp) > 0 else 0
#         recall = tp / (tp + fn) if (tp + fn) > 0 else 0
#         f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
#
#         f1_scores.append(f1)
#
#     # å½“æ‰€æœ‰ç±»å‹éƒ½ä¸å‚ä¸è®¡ç®—æ—¶è¿”å›1
#     return sum(f1_scores) / len(f1_scores) if f1_scores else 1.0
#
#
# # ğŸš€ **è¿è¡Œæ¨¡å‹ & è¯„ä¼°**
# results = existing_results  # åŠ è½½å·²å®Œæˆçš„ç»“æœ
# eval_scores = []
# total_tp = Counter()
# total_fp = Counter()
# total_fn = Counter()
# # **è¿›åº¦æ¡**
# for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
#     instruction = item["instruction"]
#     input_text = item["input"]
#     gold_answer = item["output"]
#
#     # **è°ƒç”¨ GPT-4o ç”Ÿæˆç­”æ¡ˆ**
#     model_output = get_gpt4o_response(instruction, input_text)
#
#     # æå–å®ä½“
#     gold_entities = extract_entities(gold_answer)
#     pred_entities = extract_entities(model_output)
#
#     # è®¡ç®—å®ä½“F1
#     macro_f1 = compute_entity_f1(gold_entities, pred_entities)
#
#
#     # **ä¿å­˜ç»“æœ**
#     results.append({
#         "instruction": instruction,
#         "input": input_text,
#         "gold_answer": gold_answer,
#         "model_output": model_output,
#         "entity_f1": macro_f1
#     })
#
#     eval_scores.append({
#         "entity_f1": macro_f1
#     })
#
#     # **æ¯ 50 æ¡æ•°æ®å®æ—¶ä¿å­˜ä¸€æ¬¡**
#     if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
#         with open(output_results_file, "w", encoding="utf-8") as f:
#             json.dump(results, f, ensure_ascii=False, indent=4)
#         print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")
#
# # **è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡**
# eval_scores = [item for item in results if item["model_output"] != "ERROR"]
# num_samples = len(eval_scores)
# avg_scores = {
#     "entity_f1": sum(item["entity_f1"] for item in eval_scores) / num_samples
# }
#
# # **ä¿å­˜è¯„ä¼°ç»“æœ**
# with open(eval_results_file, "w", encoding="utf-8") as f:
#     json.dump(avg_scores, f, ensure_ascii=False, indent=4)
#
# print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")


###ç»Ÿè®¡ä¸­æ–‡è¾“å‡º
# import json
# import re
#
# # åŠ è½½ JSON æ–‡ä»¶
# with open('deepseek_outputs.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # ç»Ÿè®¡åŒ…å«ä¸­æ–‡çš„ model_output æ•°æ®æ¡æ•°
# chinese_count = 0
#
# for item in data:
#     model_output = item['model_output']
#     # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ï¼ˆUnicode èŒƒå›´ \u4e00-\u9fffï¼‰
#     if re.search(r"[\u4e00-\u9fff]", model_output):
#         chinese_count += 1
#         print(model_output)
#
# # æ‰“å°ç»“æœ
# print(f"åŒ…å«ä¸­æ–‡çš„æ•°æ®æ¡æ•°: {chinese_count}")

# import os
# import json
# import unicodedata
# import re
# from tqdm import tqdm
# from collections import Counter
#
# # æ–‡ä»¶è·¯å¾„
# input_results_file = "LCTW-gpt_outputs_demo.json"
# output_results_file = "LCTW-gpt_outputs_demo2.json"
# eval_results_file = "LCTW-gpt_results_demo2.json"
#
# # åŠ è½½å·²æœ‰çš„ç»“æœæ–‡ä»¶
# if not os.path.exists(input_results_file):
#     raise FileNotFoundError(f"âŒ è¾“å…¥æ–‡ä»¶ {input_results_file} ä¸å­˜åœ¨")
# with open(input_results_file, "r", encoding="utf-8") as f:
#     results = json.load(f)
#
# def extract_entities(text):
#     """å®ä½“æå–å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰"""
#     entities = {'PER': set(), 'LOC': set(), 'ORG': set(), 'DAT': set(), 'TIM': set(), 'NUM': set(), 'EVT': set()}
#
#     pattern = r"""
#         (?i)                    # å¿½ç•¥å¤§å°å†™
#         \b(PER|LOC|ORG|DAT|TIM|NUM|EVT)\b       # å®ä½“ç±»å‹ä½œä¸ºç‹¬ç«‹å•è¯
#         \s*:\s*                 # å†’å·å‰åå¯èƒ½æœ‰ç©ºæ ¼
#         (                       # æ•è·å®ä½“å†…å®¹
#             (?:                 # éæ•è·ç»„ï¼Œç¡®ä¿ä¸è·¨å®ä½“ç±»å‹
#                 (?!\b(?:PER|LOC|ORG|DAT|TIM|NUM|EVT)\b\s*:)  # è´Ÿå‘å‰ç»ï¼Œæ’é™¤ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹
#                 .               # åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
#             )*?                 # éè´ªå©ªåŒ¹é…ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–æœ«å°¾
#         )
#         (?=\s*\b(?:PER|LOC|ORG|DAT|TIM|NUM|EVT)\b\s*:|$|\s*$)  # æ­£å‘æ–­è¨€ï¼Œç¡®ä¿åœåœ¨ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–å­—ç¬¦ä¸²æœ«å°¾
#     """
#
#     matches = re.findall(pattern, text, re.VERBOSE)
#
#     for ent_type, ent_text in matches:
#         ent_type = ent_type.upper()
#         if not ent_text or ent_text.strip().lower() in ['none']:
#             continue
#
#         cleaned = ent_text.strip().replace("'", "").replace('"', '')
#         cleaned = unicodedata.normalize('NFC', cleaned)
#         cleaned = re.sub(r'\s+', '', cleaned)
#
#         parts = [p for p in re.split(r'[,\u0f0d]', cleaned) if p]
#         filtered_parts = []
#         for part in parts:
#             if part.strip().upper() not in {'PER', 'LOC', 'ORG', 'DAT', 'TIM', 'NUM', 'EVT'}:
#                 filtered_parts.append(part)
#
#         if ent_type in entities and filtered_parts:
#             entities[ent_type].update(filtered_parts)
#
#     return entities
#
# def is_substring_match(ent1, ent2):
#     """æ£€æŸ¥ä¸¤ä¸ªå®ä½“æ˜¯å¦äº’ä¸ºåŒ…å«å…³ç³»"""
#     ent1, ent2 = ent1.strip(), ent2.strip()
#     return ent1 in ent2 or ent2 in ent1
#
# def compute_entity_f1(gold_entities, pred_entities):
#     """
#     æ”¹è¿›ç‰ˆF1è®¡ç®—ï¼Œå¤„ç†ç©ºå€¼å¹¶æ”¯æŒäº’ä¸ºåŒ…å«å…³ç³»çš„TPï¼š
#     1. å½“ä¸”ä»…å½“é»„é‡‘æ ‡å‡†æˆ–é¢„æµ‹æ ‡å‡†ä¸­æŸç±»å‹å­˜åœ¨å®ä½“æ—¶æ‰è®¡ç®—è¯¥ç±»å‹
#     2. å½“ä¸¤è€…éƒ½ä¸ºç©ºæ—¶ï¼Œè¯¥ç±»å‹ä¸å‚ä¸è®¡ç®—
#     3. TPè®¡ç®—æ—¶ï¼Œè‹¥é¢„æµ‹å®ä½“å’Œé»„é‡‘å®ä½“äº’ä¸ºåŒ…å«å…³ç³»ï¼Œè®¡ä¸º1
#     4. å®å¹³å‡åªè®¡ç®—å®é™…å‚ä¸çš„ç±»å‹
#     """
#     f1_scores = []
#     entity_types = ['PER', 'LOC', 'ORG', 'DAT', 'TIM', 'NUM', 'EVT']
#
#     for ent_type in entity_types:
#         gold_set = gold_entities.get(ent_type, set())
#         pred_set = pred_entities.get(ent_type, set())
#
#         gold_has_entities = len(gold_set) > 0
#         pred_has_entities = len(pred_set) > 0
#
#         if not gold_has_entities and not pred_has_entities:
#             continue
#
#         tp = 0
#         matched_gold = set()
#         matched_pred = set()
#
#         for gold_ent in gold_set:
#             for pred_ent in pred_set:
#                 if is_substring_match(gold_ent, pred_ent) and gold_ent not in matched_gold and pred_ent not in matched_pred:
#                     tp += 1
#                     matched_gold.add(gold_ent)
#                     matched_pred.add(pred_ent)
#                     break
#
#         fp = len(pred_set - matched_pred)
#         fn = len(gold_set - matched_gold)
#
#         precision = tp / (tp + fp) if (tp + fp) > 0 else 0
#         recall = tp / (tp + fn) if (tp + fn) > 0 else 0
#         f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
#
#         f1_scores.append(f1)
#
#     return sum(f1_scores) / len(f1_scores) if f1_scores else 1.0
#
# # ğŸš€ é‡æ–°è¯„ä¼°å¹¶æ›´æ–°F1åˆ†æ•°
# eval_scores = []
# for i, item in enumerate(tqdm(results, desc="ğŸš€ é‡æ–°è¯„ä¼°æ•°æ®", unit="æ¡")):
#     gold_answer = item["gold_answer"]
#     model_output = item["model_output"]
#
#     if model_output == "ERROR":
#         macro_f1 = 0.0
#     else:
#         gold_entities = extract_entities(gold_answer)
#         pred_entities = extract_entities(model_output)
#         macro_f1 = compute_entity_f1(gold_entities, pred_entities)
#
#     # æ›´æ–°entity_f1å­—æ®µ
#     item["entity_f1"] = macro_f1
#     eval_scores.append({
#         "input": item["input"],
#         "entity_f1": macro_f1
#     })
#
# # è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡
# num_samples = len(eval_scores)
# avg_scores = {
#     "entity_f1": sum(item["entity_f1"] for item in eval_scores) / num_samples if num_samples > 0 else 0.0,
#     "num_samples": num_samples
# }
#
# # ä¿å­˜æ›´æ–°åçš„ç»“æœ
# with open(output_results_file, "w", encoding="utf-8") as f:
#     json.dump(results, f, ensure_ascii=False, indent=4)
#
# # ä¿å­˜è¯„ä¼°ç»“æœ
# with open(eval_results_file, "w", encoding="utf-8") as f:
#     json.dump(avg_scores, f, ensure_ascii=False, indent=4)
#
# print(f"ğŸ“Š æ›´æ–°åçš„ç»“æœå·²ä¿å­˜è‡³: {output_results_file}")
# print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")
# print(f"ğŸ“ˆ å¹³å‡å®ä½“F1åˆ†æ•°: {avg_scores['entity_f1']:.4f}")
