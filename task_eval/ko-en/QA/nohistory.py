###ç¿»è¯‘æ•°æ®
# import json
# import os
# from openai import OpenAI
# from tqdm import tqdm

# # Data paths
# data_path = 'task_QA.json'
# output_file = 'task_nohistory_QA.json'


# # Load dataset
# try:
#     with open(data_path, 'r', encoding='utf-8') as f:
#         dataset = json.load(f)
# except FileNotFoundError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_path} ä¸å­˜åœ¨")
#     exit(1)
# except json.JSONDecodeError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_path} æ ¼å¼é”™è¯¯")
#     exit(1)

# # Load existing translated results (if any)
# if os.path.exists(output_file):
#     try:
#         with open(output_file, 'r', encoding='utf-8') as f:
#             translated_data = json.load(f)
#         print(f"â„¹ï¸ ç»§ç»­ä»ç¬¬ {len(translated_data)} æ¡å¼€å§‹ç¿»è¯‘")
#     except json.JSONDecodeError:
#         print(f"âŒ è¾“å‡ºæ–‡ä»¶ {output_file} æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°å¼€å§‹")
#         translated_data = []
# else:
#     translated_data = []

# # Determine the number of already translated items
# translated_count = len(translated_data)

# # Initialize OpenAI client
# client = OpenAI(api_key="0", base_url="http://0.0.0.0:8001/v1")

# # Translation function
# def translate(system_content, instruction, input_text):
#     messages = [
#         {"role": "system", "content": system_content},
#         {"role": "user", "content": f"{instruction}\n\n{input_text}"}
#     ]
#     try:
#         result = client.chat.completions.create(
#             messages=messages,
#             model="saves/qwen2.5/ko-en/qwen-ko",
#             temperature=0.6,
#             top_p=0.6,
#         )
#         return result.choices[0].message.content.strip()
#     except Exception as e:
#         print(f"âŒ ç¿»è¯‘å¤±è´¥ï¼š{input_text}ï¼Œé”™è¯¯ï¼š{str(e)}")
#         return ""

# # Translation process
# SAVE_INTERVAL = 20
# system_content = "You are a professional translator fluent in both English and Korean."
# instruction = "Translate the following sentences from Korean to English."

# for i, item in enumerate(tqdm(dataset[translated_count:], desc="ğŸ”„ ç¿»è¯‘inputå­—æ®µ", unit="æ¡", initial=translated_count, total=len(dataset))):
#     input_text = item.get('input', '')
#     if not input_text:
#         print(f"âš ï¸ ç¼ºå¤±inputå­—æ®µï¼š{item.get('id', 'æœªçŸ¥ID')}")
#         translated_data.append(item)
#         continue

#     # Translate Tibetan input to Chinese
#     input_en = translate(system_content, instruction, input_text)

#     # Retain original data and add translated input_zh
#     new_item = item.copy()
#     new_item['input_en'] = input_en
#     translated_data.append(new_item)

#     # Display progress
#     #print(f"Translated {i + 1}/{len(dataset)}: {input_en}")

#     # Save every SAVE_INTERVAL items or at the end
#     if (i + 1) % SAVE_INTERVAL == 0 or (i + 1) == len(dataset):
#         try:
#             with open(output_file, 'w', encoding='utf-8') as f:
#                 json.dump(translated_data, f, ensure_ascii=False, indent=4)
#             print(f"ğŸ’¾ å·²ä¿å­˜ {i + 1} æ¡æ•°æ®")
#         except Exception as e:
#             print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")
#             exit(1)

# # Save final results
# try:
#     with open(output_file, 'w', encoding='utf-8') as f:
#         json.dump(translated_data, f, ensure_ascii=False, indent=4)
#     print(f"\nâœ… ç¿»è¯‘å®Œæˆï¼å…±å¤„ç† {len(translated_data)} æ¡æ•°æ®")
#     print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")
# except Exception as e:
#     print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")
#     exit(1)



#####è¯„ä¼°
import json
import os
from openai import OpenAI
from transformers.generation.utils import GenerationMixin
import re
import unicodedata
from collections import Counter
from tqdm import tqdm
from mecab import MeCab
from pathlib import Path
import time
import torch
from rouge import Rouge
from sacrebleu.metrics import BLEU
# from nltk.translate.meteor_score import meteor_score


# File paths
data_file = "task_nohistory_QA.json"
output_results_file = "nohistory-doubao_outputs.json"
eval_results_file = "nohistory-doubao_results.json"

# Load dataset
try:
    with open(data_file, "r", encoding="utf-8") as f:
        dataset = json.load(f)
except FileNotFoundError:
    print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_file} ä¸å­˜åœ¨")
    exit(1)
except json.JSONDecodeError:
    print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_file} æ ¼å¼é”™è¯¯")
    exit(1)

# Load existing results (for checkpointing)
if os.path.exists(output_results_file):
    with open(output_results_file, "r", encoding="utf-8") as f:
        existing_results = json.load(f)
else:
    existing_results = []

# Deduplicate: Skip already processed data
processed_inputs = {item["input_en"] for item in existing_results if "input_en" in item}
dataset = [item for item in dataset if "input_en" in item and item["input_en"] not in processed_inputs]

# Initialize translation client
translate_client = OpenAI(api_key="0", base_url="http://0.0.0.0:8001/v1")

# Initialize inference client
# client = OpenAI(base_url="http://chatapi.littlewheat.com/v1",
#                 api_key="sk-eU6vJLbjn2cznR4sK1lV7f9zDSFARcxcbihirVfM2wOd2fiS")

client = OpenAI(base_url = "https://ark.cn-beijing.volces.com/api/v3",
                api_key  = "30f8d6d0-96d1-4334-8244-eb15bc951063")

# Translation function
def translate(system_content, instruction, input_text):
    messages = [
        {"role": "system", "content": system_content},
    ]
    messages.append({"role": "user", "content": f"{instruction}\n\n{input_text}"})
    try:
        result = translate_client.chat.completions.create(
            messages=messages,
            model="saves/qwen2.5/ko-en/qwen-ko",
            temperature=0.6,
            top_p=0.6,
        )
        return result.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¤±è´¥ï¼š{input_text}ï¼Œé”™è¯¯ï¼š{str(e)}")
        return "ERROR"

# Inference function with retry mechanism
str1="(You only need to output the answer, no extra explanation is needed)"
# **OpenAI API è°ƒç”¨**
# def get_openai_response(instruction, input_text, max_retries=3):
#     for attempt in range(max_retries):
#         messages = [
#             {"role": "system", "content": "You are an assistant who is good at answering questions"},
#             {"role": "user", "content": f"{instruction}+{str1}\n\n{input_text}"}
#         ]
#         try:
#             response = client.chat.completions.create(
#                 model="gpt-4o",
#                 messages=messages,
#                 temperature=0.6,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
#             )
#             return response.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
#             if attempt == max_retries - 1:
#                 return "ERROR"
#             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

def get_doubao_response(instruction, input_text, max_retries=3):
    for attempt in range(max_retries):
        messages = [
            {"role": "system", "content": "You are an assistant who is good at answering questions"},
            {"role": "user", "content": f"{instruction}+{str1}\n\n{input_text}"}
        ]
        try:
            response = client.chat.completions.create(
                model="doubao-pro-32k-241215",
                messages=messages,
                temperature=0.6,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
            if attempt == max_retries - 1:
                return "ERROR"
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•


# âœ… ä½¿ç”¨mecabè¿›è¡Œåˆ†è¯
# âœ… åˆå§‹åŒ– MeCab
mecab = MeCab(dictionary_path="/root/anaconda3/envs/eval/lib/python3.10/site-packages/mecab_ko_dic/dictionary")
def korean_tokenize(text):
    return " ".join(mecab.morphs(text))
# âœ… è®¡ç®— BLEU
def compute_bleu1(pred: str, gold: str) -> float:
    """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""

    # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
    bleu = BLEU(
        smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
        smooth_value=1,  # add-k çš„ k å€¼
        max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
        effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
        tokenize='ko-mecab',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
        lowercase=False  # ä¸è½¬å°å†™
    )

    # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
    # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
    bleu_score = bleu.sentence_score(pred, [gold])

    return bleu_score.score

def compute_bleu2(pred: str, gold: str) -> float:
    """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""

    # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
    bleu = BLEU(
        smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
        smooth_value=1,  # add-k çš„ k å€¼
        max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
        effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
        tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
        lowercase=False  # ä¸è½¬å°å†™
    )
    pred_tok=korean_tokenize(pred)
    gold_tok=korean_tokenize(gold)

    # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
    # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
    bleu_score = bleu.sentence_score(pred_tok, [gold_tok])

    return bleu_score.score
# âœ… è®¡ç®— ROUGE
rouge_evaluator = Rouge()
def compute_rouge(pred, gold):
    # ç©ºå€¼è¿‡æ»¤
    pred = __builtins__.str(pred).strip() or " "
    gold = __builtins__.str(gold).strip() or " "

    pred_clean = korean_tokenize(pred)#éœ€è¦åˆ†è¯
    gold_clean = korean_tokenize(gold)
    # äºŒæ¬¡éªŒè¯
    if not pred_clean.strip() or not gold_clean.strip():
        return {
            "rouge-1": {"f": 0.0},
            "rouge-2": {"f": 0.0},
            "rouge-l": {"f": 0.0}
        }

    try:
        scores = rouge_evaluator.get_scores(pred_clean, gold_clean)
        return scores[0]
    except Exception as e:
        print(f"âš ï¸ ROUGE è®¡ç®—å¤±è´¥: pred='{pred_clean}' gold='{gold_clean}'")
        return {
            "rouge-1": {"f": 0.0},
            "rouge-2": {"f": 0.0},
            "rouge-l": {"f": 0.0}
        }


# def compute_meteor(pred: str, gold: str) -> float:
#     # ä½¿ç”¨ mecab è¿›è¡ŒéŸ©è¯­åˆ†è¯
#     pred_tokens = mecab.morphs(pred)
#     gold_tokens = mecab.morphs(gold)

#     # è®¡ç®— METEOR åˆ†æ•°
#     score = meteor_score([gold_tokens], pred_tokens)
#     return score

# Main processing loop
results = existing_results
eval_scores = []

# System prompt and translation instructions
system_content = "You are a professional translator fluent in both English and Korean."
instruction_ti = "Translate the following sentences from English to Korean. "


for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
    instruction = item["instruction"]
    input_en = item["input_en"]
    input_text = item["input"]  # Original Tibetan input for history
    gold_answer = item["output"]

    # 1. Use GPT-4o for inference on instruction + input_zh
    output_en = get_doubao_response(instruction, input_en)

    # 2. Translate output_zh back to Tibetan with history

    model_output = translate(system_content, instruction_ti, output_en)

    # 3. Compute evaluation metrics
    rouge_scores = compute_rouge(model_output, gold_answer)
    bleu_scores1 = compute_bleu1(model_output, gold_answer)
    bleu_scores2 = compute_bleu2(model_output, gold_answer)
    # meteor_score_value = compute_meteor(model_output, gold_answer)

    # Save results
    results.append({
        "instruction": instruction,
        "input": input_text,
        "input_en": input_en,
        "output_en": output_en,
        "model_output": model_output,
        "gold_answer": gold_answer,
        "rouge-1": rouge_scores["rouge-1"]["f"],
        "rouge-2": rouge_scores["rouge-2"]["f"],
        "rouge-l": rouge_scores["rouge-l"]["f"],
        "bleu1": bleu_scores1,
        "bleu2": bleu_scores2,
        # "meteor": meteor_score_value  # æ–°å¢

    })

    # Save every 10 items or at the end
    if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
        try:
            with open(output_results_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            print(f"âœ… {i + 1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")


# Compute average evaluation scores
eval_scores = [item for item in results if item["model_output"] != "ERROR"]
num_samples = len(eval_scores)
if num_samples > 0:
    avg_scores = {
        "rouge-1": sum(item["rouge-1"] for item in eval_scores) / num_samples,
        "rouge-2": sum(item["rouge-2"] for item in eval_scores) / num_samples,
        "rouge-L": sum(item["rouge-l"] for item in eval_scores) / num_samples,
        "bleu1": sum(item["bleu1"] for item in eval_scores) / num_samples,
        "bleu2": sum(item["bleu2"] for item in eval_scores) / num_samples,
        # "METEOR": sum(item["meteor"] for item in eval_scores) / num_samples  # æ–°å¢
    }
else:
    avg_scores = {"rouge-1": 0.0, "rouge-2": 0.0, "rouge-L":0.0, "bleu1":0.0}

# Save evaluation results
try:
    with open(eval_results_file, "w", encoding="utf-8") as f:
        json.dump(avg_scores, f, ensure_ascii=False, indent=4)
    print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")
except Exception as e:
    print(f"âŒ è¯„ä¼°ç»“æœä¿å­˜å¤±è´¥ï¼š{str(e)}")