import os
import json
import openai
import time
import unicodedata
import re
import torch
from tqdm import tqdm
from rouge import Rouge
from collections import Counter
from openai import OpenAI # å¯¼å…¥OpenAI
from sacrebleu.metrics import BLEU
from mecab import MeCab
from pathlib import Path

# æ–‡ä»¶è·¯å¾„
data_file = "task_SUM.json"
output_results_file = "deepseek_outputs.json"
eval_results_file = "deepseek_results.json"

# åŠ è½½æ•°æ®é›†
with open(data_file, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# è¯»å–å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
if os.path.exists(output_results_file):
    with open(output_results_file, "r", encoding="utf-8") as f:
        existing_results = json.load(f)
else:
    existing_results = []

# **å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„æ•°æ®**
processed_inputs = {item["input"] for item in existing_results}
dataset = [item for item in dataset if item["input"] not in processed_inputs]

# âœ… OpenAI API å®¢æˆ·ç«¯ï¼ˆé€‚é…æ–°ç‰ˆ OpenAI SDKï¼‰
client = OpenAI(base_url = "http://chatapi.littlewheat.com/v1",
                api_key  = "sk-RQDnurboWQHgLHqMCvnn7ADAKpJdgPVTluLdL5fr6WM7Habm")
# **OpenAI API è°ƒç”¨**
def get_gpt4o_response(instruction, input_text, max_retries=3):
    for attempt in range(max_retries):
        messages = [
            {"role": "system", "content": "You are an assistant who is good at text summarization"},
            {"role": "user", "content": f"{instruction}\n\n{input_text}"}
        ]
        try:
            response = client.chat.completions.create(
                model="deepseek-r1",
                messages=messages,
                temperature=0.8,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
            if attempt == max_retries - 1:
                return "ERROR"
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•


# âœ… ä½¿ç”¨mecabè¿›è¡Œåˆ†è¯
# âœ… åˆå§‹åŒ– MeCab
mecab = MeCab(dictionary_path="/data/zhenmengyuan/miniconda3/envs/LLM/lib/mecab/dic/mecab-ko-dic/")
def korean_tokenize(text):
    return " ".join(mecab.morphs(text))
# âœ… è®¡ç®— BLEU
def compute_bleu1(pred: str, gold: str) -> float:
    """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""

    # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
    bleu = BLEU(
        smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
        smooth_value=1,  # add-k çš„ k å€¼
        max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
        effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
        tokenize='ko-mecab',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
        lowercase=False  # ä¸è½¬å°å†™
    )

    # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
    # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
    bleu_score = bleu.sentence_score(pred, [gold])

    return bleu_score.score

def compute_bleu2(pred: str, gold: str) -> float:
    """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""

    # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
    bleu = BLEU(
        smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
        smooth_value=1,  # add-k çš„ k å€¼
        max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
        effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
        tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
        lowercase=False  # ä¸è½¬å°å†™
    )
    pred_tok=korean_tokenize(pred)
    gold_tok=korean_tokenize(gold)

    # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
    # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
    bleu_score = bleu.sentence_score(pred_tok, [gold_tok])

    return bleu_score.score
# âœ… è®¡ç®— ROUGE
rouge_evaluator = Rouge()
def compute_rouge(pred, gold):
    # ç©ºå€¼è¿‡æ»¤
    pred = __builtins__.str(pred).strip() or " "
    gold = __builtins__.str(gold).strip() or " "

    pred_clean = korean_tokenize(pred)#éœ€è¦åˆ†è¯
    gold_clean = korean_tokenize(gold)
    # äºŒæ¬¡éªŒè¯
    if not pred_clean.strip() or not gold_clean.strip():
        return {
            "rouge-1": {"f": 0.0},
            "rouge-2": {"f": 0.0},
            "rouge-l": {"f": 0.0}
        }

    try:
        scores = rouge_evaluator.get_scores(pred_clean, gold_clean)
        return scores[0]
    except Exception as e:
        print(f"âš ï¸ ROUGE è®¡ç®—å¤±è´¥: pred='{pred_clean}' gold='{gold_clean}'")
        return {
            "rouge-1": {"f": 0.0},
            "rouge-2": {"f": 0.0},
            "rouge-l": {"f": 0.0}
        }

# ğŸš€ **è¿è¡Œæ¨¡å‹ & è¯„ä¼°**
results = existing_results  # åŠ è½½å·²å®Œæˆçš„ç»“æœ
eval_scores = []

# **è¿›åº¦æ¡**
for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
    instruction = item["instruction"]
    input_text = item["input"]
    gold_answer = item["output"]

    # **è°ƒç”¨ GPT-4o ç”Ÿæˆç­”æ¡ˆ**
    model_output = get_gpt4o_response(instruction, input_text)

    # **è®¡ç®—è¯„ä¼°æŒ‡æ ‡**
    rouge_scores = compute_rouge(model_output, gold_answer)
    bleu_scores1 = compute_bleu1(model_output, gold_answer)
    bleu_scores2 = compute_bleu2(model_output, gold_answer)


    # **ä¿å­˜ç»“æœ**
    results.append({
        "instruction": instruction,
        "input": input_text,
        "gold_answer": gold_answer,
        "model_output": model_output,
        "rouge-1": rouge_scores["rouge-1"]["f"],
        "rouge-2": rouge_scores["rouge-2"]["f"],
        "rouge-l": rouge_scores["rouge-l"]["f"],
        "bleu1": bleu_scores1,
        "bleu2": bleu_scores2
    })

    eval_scores.append({
        "rouge-1": rouge_scores["rouge-1"]["f"],
        "rouge-2": rouge_scores["rouge-2"]["f"],
        "rouge-l": rouge_scores["rouge-l"]["f"],
        "bleu1": bleu_scores1,
        "bleu2": bleu_scores2
    })

    # **æ¯ 50 æ¡æ•°æ®å®æ—¶ä¿å­˜ä¸€æ¬¡**
    if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
        with open(output_results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")

# **è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡**
eval_scores = [item for item in results if item["model_output"] != "ERROR"]
num_samples = len(eval_scores)
avg_scores = {
    "ROUGE-1": sum(item["rouge-1"] for item in eval_scores) / num_samples,
    "ROUGE-2": sum(item["rouge-2"] for item in eval_scores) / num_samples,
    "ROUGE-L": sum(item["rouge-l"] for item in eval_scores) / num_samples,
    "BLEU1": sum(item["bleu1"] for item in eval_scores) / num_samples,
    "BLEU2": sum(item["bleu2"] for item in eval_scores) / num_samples
}

# **ä¿å­˜è¯„ä¼°ç»“æœ**
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(avg_scores, f, ensure_ascii=False, indent=4)

print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")