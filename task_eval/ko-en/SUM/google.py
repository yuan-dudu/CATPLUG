###ä¸€ã€è°·æ­Œç¿»è¯‘
# import json
# from openpyxl import Workbook
#
# # è¯»å–JSONæ–‡ä»¶
# with open('task_SUM.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # åˆ›å»ºä¸¤ä¸ªExcelå·¥ä½œç°¿
# wb_zh = Workbook()
# ws_zh = wb_zh.active
#
#
# # å¤„ç†å¹¶å†™å…¥æ•°æ®
# for item in data:
#     input_text = item.get('input', '')
#     ws_zh.append([input_text])
#
# # ä¿å­˜Excelæ–‡ä»¶
# wb_zh.save('en.xlsx')
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼šen.xlsx")

##äºŒã€æ•´ç†æ•°æ®
# import json
# from openpyxl import load_workbook
#
# # è¯»å–åŸå§‹JSONæ–‡ä»¶
# with open('task_SUM.json', 'r', encoding='utf-8') as f:
#     mrc_data = json.load(f)
#
# # è¯»å–zh.xlsxæ–‡ä»¶
# wb_zh = load_workbook('en.xlsx')
# ws_zh = wb_zh.active
# zh_list = [row[0].value if row[0].value is not None else "" for row in ws_zh.iter_rows(min_row=1)]
#
# # éªŒè¯æ•°æ®é•¿åº¦ä¸€è‡´æ€§
# if len(mrc_data) != len(zh_list):
#     raise ValueError("JSONå’ŒExcelæ•°æ®æ¡æ•°ä¸ä¸€è‡´")
#
# # æ„å»ºæ–°æ•°æ®ç»“æ„
# result = []
# for mrc_item, en_part in zip(mrc_data, zh_list):
#     input_en = en_part
#
#     new_item = {
#         "instruction": mrc_item["instruction"],
#         "input": mrc_item['input'],
#         "output": mrc_item['output'],
#         "input_en": input_en
#     }
#     result.append(new_item)
#
# # å†™å…¥æ–°JSONæ–‡ä»¶
# with open('task_SUM_google.json', 'w', encoding='utf-8') as f:
#     json.dump(result, f, ensure_ascii=False, indent=4)
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼štask_SUM_google.json")

###ä¸‰ã€æ¨¡å‹æ¨ç†
# import os
# import json
# import openai
# import time
# import unicodedata
# import re
# import torch
# from tqdm import tqdm
# from rouge import Rouge
# from collections import Counter
# from openai import OpenAI # å¯¼å…¥OpenAI
# from sacrebleu.metrics import BLEU
# import botok
# from botok import WordTokenizer
# from botok.config import Config
# from pathlib import Path
# import requests
# from tenacity import retry, stop_after_attempt, wait_exponential
# from openpyxl import Workbook
#
# # æ–‡ä»¶è·¯å¾„
# data_file = "task_SUM_google.json"
# output_results_file = "google_gpt_outputs.json"
#
# # åŠ è½½æ•°æ®é›†
# with open(data_file, "r", encoding="utf-8") as f:
#     dataset = json.load(f)
#
# # è¯»å–å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
# if os.path.exists(output_results_file):
#     with open(output_results_file, "r", encoding="utf-8") as f:
#         existing_results = json.load(f)
# else:
#     existing_results = []
#
# # **å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„æ•°æ®**
# processed_inputs = {item["input_en"] for item in existing_results}
# dataset = [item for item in dataset if item["input_en"] not in processed_inputs]
#
# # âœ… OpenAI API å®¢æˆ·ç«¯ï¼ˆé€‚é…æ–°ç‰ˆ OpenAI SDKï¼‰
# client = OpenAI(base_url="http://chatapi.littlewheat.com/v1",
#                 api_key="sk-RQDnurboWQHgLHqMCvnn7ADAKpJdgPVTluLdL5fr6WM7Habm")
# # client = OpenAI(base_url = "https://ark.cn-beijing.volces.com/api/v3",
# #                 api_key  = "aa1ce03a-6b61-46fc-b914-73fae89921f3")
#
# # **OpenAI API è°ƒç”¨**
# def get_openai_response(instruction, input_text, max_retries=3):
#     for attempt in range(max_retries):
#         messages = [
#             {"role": "system", "content": "You are an assistant who is good at text summarization"},
#             {"role": "user", "content": f"{instruction}\n\n{input_text}"}
#         ]
#         try:
#             response = client.chat.completions.create(
#                 model="gpt-4o",
#                 messages=messages,
#                 temperature=0.6,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
#             )
#             return response.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
#             if attempt == max_retries - 1:
#                 return "ERROR"
#             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
#
# # def get_doubao_response(instruction, input_text, max_retries=3):
# #     for attempt in range(max_retries):
# #         messages = [
# #             {"role": "system", "content": "You are an assistant who is good at text summarization"},
# #             {"role": "user", "content": f"{instruction}\n\n{input_text}"}
# #         ]
# #         try:
# #             response = client.chat.completions.create(
# #                 model="doubao-pro-32k-241215",
# #                 messages=messages,
# #                 temperature=0.6  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
# #             )
# #             return response.choices[0].message.content.strip()
# #         except Exception as e:
# #             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
# #             if attempt == max_retries - 1:
# #                 return "ERROR"
# #             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
#
# # ğŸš€ **è¿è¡Œæ¨¡å‹ & è¯„ä¼°**
# results = existing_results  # åŠ è½½å·²å®Œæˆçš„ç»“æœ
# eval_scores = []
# # **è¿›åº¦æ¡**
# for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
#     instruction = item["instruction"]
#     input_text = item["input"]
#     input_en = item["input_en"]
#     gold_answer = item["output"]
#
#     # **è°ƒç”¨ GPT-4o ç”Ÿæˆç­”æ¡ˆ**
#     output_en = get_openai_response(instruction, input_en)
#
#     # **ä¿å­˜ç»“æœ**
#     results.append({
#         "instruction": instruction,
#         "input": input_text,
#         "input_en": input_en,
#         "output_en": output_en,
#         "gold_answer": gold_answer
#     })
#     # **æ¯ 50 æ¡æ•°æ®å®æ—¶ä¿å­˜ä¸€æ¬¡**
#     if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
#         with open(output_results_file, "w", encoding="utf-8") as f:
#             json.dump(results, f, ensure_ascii=False, indent=4)
#         print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")
#
#
# # è¯»å–JSONæ–‡ä»¶
# with open('google_gpt_outputs.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # åˆ›å»ºä¸¤ä¸ªExcelå·¥ä½œç°¿
# wb_zh = Workbook()
# ws_zh = wb_zh.active
#
#
# # å¤„ç†å¹¶å†™å…¥æ•°æ®
# for item in data:
#     input_text = item.get('output_en', '')
#     ws_zh.append([input_text])
#
# # ä¿å­˜Excelæ–‡ä»¶
# wb_zh.save('ko_gpt.xlsx')
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼ško_gpt.xlsx")


####æ•´ç†æ•°æ®
# import json
# from openpyxl import load_workbook
#
# # è¯»å–åŸå§‹JSONæ–‡ä»¶
# with open('google_gpt_outputs.json', 'r', encoding='utf-8') as f:
#     mrc_data = json.load(f)
#
# # è¯»å–zh.xlsxæ–‡ä»¶
# wb_eh = load_workbook('ko_gpt.xlsx')
# ws_eh = wb_eh.active
# eh_list = [row[0].value if row[0].value is not None else "" for row in ws_eh.iter_rows(min_row=1)]
#
#
# # éªŒè¯æ•°æ®é•¿åº¦ä¸€è‡´æ€§
# if len(mrc_data) != len(eh_list):
#     raise ValueError("JSONå’ŒExcelæ•°æ®æ¡æ•°ä¸ä¸€è‡´")
#
# # æ„å»ºæ–°æ•°æ®ç»“æ„
# result = []
# for mrc_item, eh_part in zip(mrc_data, eh_list):
#     input_en = eh_part
#
#     new_item = {
#         "instruction": mrc_item['instruction'],
#         "input": mrc_item['input'],
#         "gold_answer": mrc_item['gold_answer'],
#         "input_en": mrc_item["input_en"],
#         "output_en": mrc_item["output_en"],
#         "model_output": input_en
#     }
#     result.append(new_item)
#
# # å†™å…¥æ–°JSONæ–‡ä»¶
# with open('google_gpt_outputs2.json', 'w', encoding='utf-8') as f:
#     json.dump(result, f, ensure_ascii=False, indent=4)
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼šgoogle_gpt_outputs2.json")



####è¯„ä¼°
import os
import json
import openai
import time
import unicodedata
import re
import torch
from tqdm import tqdm
from rouge import Rouge
from collections import Counter
from openai import OpenAI # å¯¼å…¥OpenAI
from sacrebleu.metrics import BLEU
import botok
from botok import WordTokenizer
from botok.config import Config
from pathlib import Path
from sentence_transformers import SentenceTransformer, util
from transformers.generation.utils import GenerationMixin
from mecab import MeCab

# æ–‡ä»¶è·¯å¾„
input_file = "google_gpt_outputs.json"  # è¾“å…¥æ–‡ä»¶
output_file = "google_gpt_outputs.json"  # ç»“æœå†™å›åŸæ–‡ä»¶
eval_results_file = "google_gpt_results.json"  # æœ€ç»ˆè¯„ä¼°ç»“æœ


# è¯»å–å·²æœ‰æ•°æ®ï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
if os.path.exists(input_file):
    with open(input_file, "r", encoding="utf-8") as f:
        dataset = json.load(f)
else:
    raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")

# âœ… ä½¿ç”¨botokè¿›è¡Œåˆ†è¯
config = Config(dialect_name="general", base_path=Path.home())
tokenizer = WordTokenizer(config=config)
def segment_tibetan_text(text):
    """ä½¿ç”¨ Botok è¿›è¡Œè—æ–‡åˆ†è¯"""
    tokens = tokenizer.tokenize(text, split_affixes=False)
    return " ".join([token.text for token in tokens])  # è¿”å›ç©ºæ ¼åˆ†éš”çš„åˆ†è¯ç»“æœ

def tibetan_syllable_segment(text):
    """ä½¿ç”¨è—è¯­éŸ³èŠ‚ç¬¦à¼‹è¿›è¡Œåˆ†è¯"""
    # åœ¨æ¯ä¸ªéŸ³èŠ‚ç¬¦åæ·»åŠ ç©ºæ ¼ï¼Œåˆå¹¶å¤šä½™ç©ºæ ¼ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    segmented = text.replace('à¼‹', 'à¼‹ ')
    segmented = re.sub(r' +', ' ', segmented)  # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼
    return segmented.strip()

# âœ… è®¡ç®— BLEU
def compute_bleu1(pred: str, gold: str) -> float:
    """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""

    # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
    bleu = BLEU(
        smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
        smooth_value=1,  # add-k çš„ k å€¼
        max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
        effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
        tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
        lowercase=False  # ä¸è½¬å°å†™
    )

    # âœ… è—æ–‡åˆ†è¯ï¼ˆå‡è®¾ tibetan_syllable_segment å·²æ­£ç¡®å®šä¹‰ï¼‰
    pred_tok = segment_tibetan_text(pred)
    gold_tok = segment_tibetan_text(gold)

    # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
    # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
    bleu_score = bleu.sentence_score(pred_tok, [gold_tok])

    return bleu_score.score

# âœ… è®¡ç®— BLEU
def compute_bleu2(pred: str, gold: str) -> float:
    """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""

    # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
    bleu = BLEU(
        smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
        smooth_value=1,  # add-k çš„ k å€¼
        max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
        effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
        tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
        lowercase=False  # ä¸è½¬å°å†™
    )

    # âœ… è—æ–‡åˆ†è¯ï¼ˆå‡è®¾ tibetan_syllable_segment å·²æ­£ç¡®å®šä¹‰ï¼‰
    pred_tok = tibetan_syllable_segment(pred)
    gold_tok = tibetan_syllable_segment(gold)

    # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
    # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
    bleu_score = bleu.sentence_score(pred_tok, [gold_tok])

    return bleu_score.score
# âœ… è®¡ç®— ROUGE
rouge_evaluator = Rouge()
def compute_rouge(pred, gold):
    # ç©ºå€¼è¿‡æ»¤
    pred = __builtins__.str(pred).strip() or " "
    gold = __builtins__.str(gold).strip() or " "

    pred_clean = segment_tibetan_text(pred)#éœ€è¦åˆ†è¯
    gold_clean = segment_tibetan_text(gold)
    # äºŒæ¬¡éªŒè¯
    if not pred_clean.strip() or not gold_clean.strip():
        return {
            "rouge-1": {"f": 0.0},
            "rouge-2": {"f": 0.0},
            "rouge-l": {"f": 0.0}
        }

    try:
        scores = rouge_evaluator.get_scores(pred_clean, gold_clean)
        return scores[0]
    except Exception as e:
        print(f"âš ï¸ ROUGE è®¡ç®—å¤±è´¥: pred='{pred_clean}' gold='{gold_clean}'")
        return {
            "rouge-1": {"f": 0.0},
            "rouge-2": {"f": 0.0},
            "rouge-l": {"f": 0.0}
        }



processed_count = 0
for item in tqdm(dataset, desc="ğŸš€ è®¡ç®—å®ä½“F1", unit="æ¡"):
    # è·³è¿‡å·²è®¡ç®—è¿‡çš„æ¡ç›®
    if "rouge-1" in item:
        continue

    gold_answer = item["gold_answer"]
    model_output = item["model_output"]

    # æå–å®ä½“å¹¶è®¡ç®—F1
    rouge_scores = compute_rouge(model_output, gold_answer)
    bleu_scores1 = compute_bleu1(model_output, gold_answer)
    bleu_scores2 = compute_bleu2(model_output, gold_answer)

    # å°†ç»“æœå†™å…¥åŸæ•°æ®
    item["rouge-1"] = rouge_scores["rouge-1"]["f"]
    item["rouge-2"]= rouge_scores["rouge-2"]["f"]
    item["rouge-L"]= rouge_scores["rouge-l"]["f"]
    item["bleu1"] =bleu_scores1
    item["bleu2"] = bleu_scores2
    processed_count += 1

    # æ¯å¤„ç†10æ¡ä¿å­˜ä¸€æ¬¡
    if processed_count % 10 == 0:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(dataset, f, ensure_ascii=False, indent=4)
        print(f"âœ… å·²ä¿å­˜ {processed_count} æ¡è®¡ç®—ç»“æœ")

# æœ€ç»ˆä¿å­˜å®Œæ•´æ•°æ®
with open(output_file, "w", encoding="utf-8") as f:
    json.dump(dataset, f, ensure_ascii=False, indent=4)
print(f"âœ… æ‰€æœ‰ {len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_file}")

# è®¡ç®—å¹³å‡F1
valid_items = [item for item in dataset if "rouge-1" in item]
avg_rouge1 = sum(item["rouge-1"] for item in valid_items) / len(valid_items)
avg_rouge2 = sum(item["rouge-2"] for item in valid_items) / len(valid_items)
avg_rougel = sum(item["rouge-L"] for item in valid_items) / len(valid_items)
avg_bleu1 = sum(item["bleu1"] for item in valid_items) / len(valid_items)
avg_bleu2 = sum(item["bleu2"] for item in valid_items) / len(valid_items)

# ä¿å­˜è¯„ä¼°ç»“æœ
results = {
    "rouge-1": avg_rouge1,
    "rouge-2": avg_rouge2,
    "rouge-l": avg_rougel,
    "bleu1": avg_bleu1,
    "bleu2": avg_bleu2
}
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=4)

print(f"""
ğŸ“Š è¯„ä¼°ç»“æœï¼š
- rouge-1ï¼š{avg_rouge1:.4f}
- rouge-2ï¼š{avg_rouge2:.4f}
- rouge-lï¼š{avg_rougel:.4f}
- bleu1ï¼š{avg_bleu1:.4f}
- bleu2ï¼š{avg_bleu2:.4f}
- ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{eval_results_file}
""")