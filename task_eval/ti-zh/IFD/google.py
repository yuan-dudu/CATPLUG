####ä¸€ã€è°·æ­Œç¿»è¯‘
# import json
# from openpyxl import Workbook
#
# # è¯»å–JSONæ–‡ä»¶
# with open('task_NTG.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # åˆ›å»ºä¸¤ä¸ªExcelå·¥ä½œç°¿
# wb_zh = Workbook()
# ws_zh = wb_zh.active
#
#
# # å¤„ç†å¹¶å†™å…¥æ•°æ®
# for item in data:
#     input_text = item.get('input', '')
#     ws_zh.append([input_text])
#
# # ä¿å­˜Excelæ–‡ä»¶
# wb_zh.save('zh.xlsx')
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼šzh.xlsx")

###äºŒã€æ•´ç†æ•°æ®
# import json
# from openpyxl import load_workbook
#
# # è¯»å–åŸå§‹JSONæ–‡ä»¶
# with open('task_NTG.json', 'r', encoding='utf-8') as f:
#     mrc_data = json.load(f)
#
# # è¯»å–zh.xlsxæ–‡ä»¶
# wb_zh = load_workbook('zh.xlsx')
# ws_zh = wb_zh.active
# zh_list = [row[0].value if row[0].value is not None else "" for row in ws_zh.iter_rows(min_row=1)]
#
#
# # éªŒè¯æ•°æ®é•¿åº¦ä¸€è‡´æ€§
# if len(mrc_data) != len(zh_list):
#     raise ValueError("JSONå’ŒExcelæ•°æ®æ¡æ•°ä¸ä¸€è‡´")
#
# # æ„å»ºæ–°æ•°æ®ç»“æ„
# result = []
# for mrc_item, zh_part in zip(mrc_data, zh_list):
#     # æ‹¼æ¥zh.xlsxå’ŒQ.xlsxçš„å†…å®¹ï¼Œä¸­é—´ç”¨\nè¿æ¥
#     input_zh = zh_part
#
#     new_item = {
#         "instruction": mrc_item['instruction'],
#         "input": mrc_item['input'],
#         "output": mrc_item['output'],
#         "input_zh": input_zh
#     }
#     result.append(new_item)
#
# # å†™å…¥æ–°JSONæ–‡ä»¶
# with open('task_NTG_google.json', 'w', encoding='utf-8') as f:
#     json.dump(result, f, ensure_ascii=False, indent=4)
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼štask_NER_google.json")



import os
import json
import openai
import time
import unicodedata
import re
import torch
from tqdm import tqdm
from rouge import Rouge
from transformers.generation.utils import GenerationMixin
from collections import Counter
from openai import OpenAI # å¯¼å…¥OpenAI
from sacrebleu.metrics import BLEU
import botok
from botok import WordTokenizer
from botok.config import Config
from pathlib import Path
import requests
from tenacity import retry, stop_after_attempt, wait_exponential

# æ–‡ä»¶è·¯å¾„
data_file = "task_NTG_google.json"
output_results_file = "google_gpt_outputs.json"
eval_results_file = "google_gpt_results.json"

# åŠ è½½æ•°æ®é›†
with open(data_file, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# è¯»å–å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
if os.path.exists(output_results_file):
    with open(output_results_file, "r", encoding="utf-8") as f:
        existing_results = json.load(f)
else:
    existing_results = []

# # **å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„æ•°æ®**
# processed_inputs = {item["input_zh"] for item in existing_results}
# dataset = [item for item in dataset if item["input_zh"] not in processed_inputs]

# # âœ… OpenAI API å®¢æˆ·ç«¯ï¼ˆé€‚é…æ–°ç‰ˆ OpenAI SDKï¼‰
# client = OpenAI(base_url="http://chatapi.littlewheat.com/v1",
#                 api_key="sk-RQDnurboWQHgLHqMCvnn7ADAKpJdgPVTluLdL5fr6WM7Habm")
# # client = OpenAI(base_url = "https://ark.cn-beijing.volces.com/api/v3",
# #                 api_key  = "aa1ce03a-6b61-46fc-b914-73fae89921f3")
#
# str1="åªéœ€è¾“å‡ºç®€çŸ­æ–°é—»æ ‡é¢˜)"
# # **OpenAI API è°ƒç”¨**
# def get_openai_response(instruction, input_text, max_retries=3):
#     for attempt in range(max_retries):
#         messages = [
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–°é—»æ ‡é¢˜ç”Ÿæˆçš„åŠ©æ‰‹"},
#             {"role": "user", "content": f"{instruction}+{str1}\n\n{input_text}"}
#         ]
#         try:
#             response = client.chat.completions.create(
#                 model="gpt-4o",
#                 messages=messages,
#                 temperature=0.6,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
#             )
#             return response.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
#             if attempt == max_retries - 1:
#                 return "ERROR"
#             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
#
# # def get_doubao_response(instruction, input_text, max_retries=3):
# #     for attempt in range(max_retries):
# #         messages = [
# #             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–°é—»æ ‡é¢˜ç”Ÿæˆçš„åŠ©æ‰‹"},
# #             {"role": "user", "content": f"{instruction}+{str1}\n\n{input_text}"}
# #         ]
# #         try:
# #             response = client.chat.completions.create(
# #                 model="doubao-pro-32k-241215",
# #                 messages=messages,
# #                 temperature=0.6  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
# #             )
# #             return response.choices[0].message.content.strip()
# #         except Exception as e:
# #             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
# #             if attempt == max_retries - 1:
# #                 return "ERROR"
# #             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
#
# # âœ… ä½¿ç”¨botokè¿›è¡Œåˆ†è¯
# config = Config(dialect_name="general", base_path=Path.home())
# tokenizer = WordTokenizer(config=config)
# def segment_tibetan_text(text):
#     """ä½¿ç”¨ Botok è¿›è¡Œè—æ–‡åˆ†è¯"""
#     tokens = tokenizer.tokenize(text, split_affixes=False)
#     return " ".join([token.text for token in tokens])  # è¿”å›ç©ºæ ¼åˆ†éš”çš„åˆ†è¯ç»“æœ
#
# def tibetan_syllable_segment(text):
#     """ä½¿ç”¨è—è¯­éŸ³èŠ‚ç¬¦à¼‹è¿›è¡Œåˆ†è¯"""
#     # åœ¨æ¯ä¸ªéŸ³èŠ‚ç¬¦åæ·»åŠ ç©ºæ ¼ï¼Œåˆå¹¶å¤šä½™ç©ºæ ¼ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
#     segmented = text.replace('à¼‹', 'à¼‹ ')
#     segmented = re.sub(r' +', ' ', segmented)  # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼
#     return segmented.strip()
#
# # âœ… è®¡ç®— BLEU
# def compute_bleu1(pred: str, gold: str) -> float:
#     """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""
#
#     # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
#     bleu = BLEU(
#         smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
#         smooth_value=1,  # add-k çš„ k å€¼
#         max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
#         effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
#         tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
#         lowercase=False  # ä¸è½¬å°å†™
#     )
#
#     # âœ… è—æ–‡åˆ†è¯ï¼ˆå‡è®¾ tibetan_syllable_segment å·²æ­£ç¡®å®šä¹‰ï¼‰
#     pred_tok = segment_tibetan_text(pred)
#     gold_tok = segment_tibetan_text(gold)
#
#     # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
#     # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
#     bleu_score = bleu.sentence_score(pred_tok, [gold_tok])
#
#     return bleu_score.score
#
# # âœ… è®¡ç®— BLEU
# def compute_bleu2(pred: str, gold: str) -> float:
#     """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""
#
#     # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
#     bleu = BLEU(
#         smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
#         smooth_value=1,  # add-k çš„ k å€¼
#         max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
#         effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
#         tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
#         lowercase=False  # ä¸è½¬å°å†™
#     )
#
#     # âœ… è—æ–‡åˆ†è¯ï¼ˆå‡è®¾ tibetan_syllable_segment å·²æ­£ç¡®å®šä¹‰ï¼‰
#     pred_tok = tibetan_syllable_segment(pred)
#     gold_tok = tibetan_syllable_segment(gold)
#
#     # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
#     # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
#     bleu_score = bleu.sentence_score(pred_tok, [gold_tok])
#
#     return bleu_score.score
# # âœ… è®¡ç®— ROUGE
# rouge_evaluator = Rouge()
# def compute_rouge(pred, gold):
#     # ç©ºå€¼è¿‡æ»¤
#     pred = __builtins__.str(pred).strip() or " "
#     gold = __builtins__.str(gold).strip() or " "
#
#     pred_clean = segment_tibetan_text(pred)#éœ€è¦åˆ†è¯
#     gold_clean = segment_tibetan_text(gold)
#     # äºŒæ¬¡éªŒè¯
#     if not pred_clean.strip() or not gold_clean.strip():
#         return {
#             "rouge-1": {"f": 0.0},
#             "rouge-2": {"f": 0.0},
#             "rouge-l": {"f": 0.0}
#         }
#
#     try:
#         scores = rouge_evaluator.get_scores(pred_clean, gold_clean)
#         return scores[0]
#     except Exception as e:
#         print(f"âš ï¸ ROUGE è®¡ç®—å¤±è´¥: pred='{pred_clean}' gold='{gold_clean}'")
#         return {
#             "rouge-1": {"f": 0.0},
#             "rouge-2": {"f": 0.0},
#             "rouge-l": {"f": 0.0}
#         }
#
#
# # Translation function
# API_KEY = "ymg9YDPBCofY1DxJqxy6"
# API_URL = "http://www.trans-home.com/api/index/translate"
#
# @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
# def translate_to_tibetan(text):
#     if not text or not isinstance(text, str):
#         return ""
#     try:
#         params = {"token": API_KEY}
#         data = {
#             "keywords": text,
#             "sourceLanguage": "zh-cn",  # Fixed to translate from Chinese to Tibetan
#             "targetLanguage": "bo"
#         }
#         response = requests.post(API_URL, params=params, json=data, timeout=10)
#         response.raise_for_status()
#         result = response.json()
#         if result.get("code") == 1 and "data" in result and "text" in result["data"]:
#             return result["data"]["text"]
#         else:
#             print(f"âŒ ç¿»è¯‘å¤±è´¥ï¼š{text}ï¼Œé”™è¯¯ï¼š{result.get('info', 'æœªçŸ¥é”™è¯¯')}")
#             return ""
#     except Exception as e:
#         print(f"âŒ ç¿»è¯‘å¼‚å¸¸ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
#         raise

# ğŸš€ **è¿è¡Œæ¨¡å‹ & è¯„ä¼°**
results = existing_results  # åŠ è½½å·²å®Œæˆçš„ç»“æœ
# eval_scores = []
# # **è¿›åº¦æ¡**
# for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
#     instruction = item["instruction"]
#     input_text = item["input"]
#     input_zh = item["input_zh"]
#     gold_answer = item["output"]
#
#     # **è°ƒç”¨ GPT-4o ç”Ÿæˆç­”æ¡ˆ**
#     output_zh = get_openai_response(instruction, input_zh)
#
#     # Translate to Tibetan
#     model_output = translate_to_tibetan(output_zh)
#
#     # è®¡ç®—å®ä½“F1
#     rouge_scores = compute_rouge(model_output, gold_answer)
#     bleu_scores1 = compute_bleu1(model_output, gold_answer)
#     bleu_scores2 = compute_bleu2(model_output, gold_answer)
#
#
#     # **ä¿å­˜ç»“æœ**
#     results.append({
#         "instruction": instruction,
#         "input": input_text,
#         "input_zh": input_zh,
#         "output_zh": output_zh,
#         "gold_answer": gold_answer,
#         "model_output": model_output,
#         "rouge-1": rouge_scores["rouge-1"]["f"],
#         "rouge-2": rouge_scores["rouge-2"]["f"],
#         "rouge-l": rouge_scores["rouge-l"]["f"],
#         "bleu1": bleu_scores1,
#         "bleu2": bleu_scores2
#     })
#     # **æ¯ 50 æ¡æ•°æ®å®æ—¶ä¿å­˜ä¸€æ¬¡**
#     if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
#         with open(output_results_file, "w", encoding="utf-8") as f:
#             json.dump(results, f, ensure_ascii=False, indent=4)
#         print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")

# **è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡**
eval_scores = [item for item in results if item["model_output"] != "ERROR"]
num_samples = len(eval_scores)
avg_scores = {
        "rouge-1": sum(item["rouge-1"] for item in eval_scores) / num_samples,
        "rouge-2": sum(item["rouge-2"] for item in eval_scores) / num_samples,
        "rouge-L": sum(item["rouge-l"] for item in eval_scores) / num_samples,
        "bleu1": sum(item["bleu1"] for item in eval_scores) / num_samples,
        "bleu2": sum(item["bleu2"] for item in eval_scores) / num_samples
}

# **ä¿å­˜è¯„ä¼°ç»“æœ**
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(avg_scores, f, ensure_ascii=False, indent=4)

print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")