####ä¸€ã€è°·æ­Œç¿»è¯‘
# import json
# from openpyxl import Workbook
#
# # è¯»å–JSONæ–‡ä»¶
# with open('task_MRC.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # åˆ›å»ºä¸¤ä¸ªExcelå·¥ä½œç°¿
# wb_zh = Workbook()
# ws_zh = wb_zh.active
#
# wb_q = Workbook()
# ws_q = wb_q.active
#
# # å¤„ç†å¹¶å†™å…¥æ•°æ®
# for item in data:
#     input_text = item.get('input', '')
#
#     # ä»åå¾€å‰æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦
#     last_newline = input_text.rfind('\n')
#
#     if last_newline != -1:
#         # åˆ†éš”æ–‡æœ¬
#         text_part = input_text[:last_newline]  # \nå‰é¢çš„éƒ¨åˆ†
#         question_part = input_text[last_newline + 1:]  # \nåé¢çš„éƒ¨åˆ†
#
#         # åˆ†åˆ«å†™å…¥ä¸¤ä¸ªExcelæ–‡ä»¶
#         ws_zh.append([text_part])
#         ws_q.append([question_part])
#     else:
#         # å¦‚æœæ²¡æœ‰æ¢è¡Œç¬¦ï¼Œå…¨æ–‡å†™å…¥zh.xlsxï¼ŒQ.xlsxå†™å…¥ç©ºè¡Œ
#         ws_zh.append([input_text])
#         ws_q.append([''])
#
# # ä¿å­˜Excelæ–‡ä»¶
# wb_zh.save('zh.xlsx')
# wb_q.save('Q.xlsx')
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼šzh.xlsx å’Œ Q.xlsx")

###äºŒã€æ•´ç†æ•°æ®
# import json
# from openpyxl import load_workbook
#
# # è¯»å–åŸå§‹JSONæ–‡ä»¶
# with open('task_MRC.json', 'r', encoding='utf-8') as f:
#     mrc_data = json.load(f)
#
# # è¯»å–zh.xlsxæ–‡ä»¶
# wb_zh = load_workbook('zh.xlsx')
# ws_zh = wb_zh.active
# zh_list = [row[0].value if row[0].value is not None else "" for row in ws_zh.iter_rows(min_row=1)]
#
# # è¯»å–Q.xlsxæ–‡ä»¶
# wb_q = load_workbook('Q.xlsx')
# ws_q = wb_q.active
# q_list = [row[0].value if row[0].value is not None else "" for row in ws_q.iter_rows(min_row=1)]
#
# # éªŒè¯æ•°æ®é•¿åº¦ä¸€è‡´æ€§
# if len(mrc_data) != len(zh_list) or len(mrc_data) != len(q_list):
#     raise ValueError("JSONå’ŒExcelæ•°æ®æ¡æ•°ä¸ä¸€è‡´")
#
# # æ„å»ºæ–°æ•°æ®ç»“æ„
# result = []
# for mrc_item, zh_part, q_part in zip(mrc_data, zh_list, q_list):
#     # æ‹¼æ¥zh.xlsxå’ŒQ.xlsxçš„å†…å®¹ï¼Œä¸­é—´ç”¨\nè¿æ¥
#     input_zh = f"{zh_part}\n{q_part}" if q_part else zh_part
#
#     new_item = {
#         "instruction": "è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å®Œæˆé˜…è¯»ç†è§£ä»»åŠ¡ï¼Œåªéœ€æ ¹æ®æœ€åçš„é—®é¢˜è¾“å‡ºç®€æ´çš„ç­”æ¡ˆã€‚",
#         "input": mrc_item['input'],
#         "output": mrc_item['output'],
#         "input_zh": input_zh
#     }
#     result.append(new_item)
#
# # å†™å…¥æ–°JSONæ–‡ä»¶
# with open('task_MRC_zh.json', 'w', encoding='utf-8') as f:
#     json.dump(result, f, ensure_ascii=False, indent=4)
#
# print("æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼štask_MRC_zh.json")


####dè°ƒç”¨è°·æ­Œè¿›è¡Œç¿»è¯‘
# import json
# import requests
# from tqdm import tqdm
# import os
# from tenacity import retry, stop_after_attempt, wait_exponential
# # API é…ç½®
# API_KEY = "ymg9YDPBCofY1DxJqxy6"
# API_URL = "http://www.trans-home.com/api/index/translate"
#
# # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šé‡è¯•3æ¬¡ï¼Œç­‰å¾…æ—¶é—´æŒ‡æ•°å¢é•¿
# @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
# def translate_to_tibetan(text):
#     """å°†ä¸­æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºè—æ–‡"""
#     if not text or not isinstance(text, str):
#         return ""
#     try:
#         params = {"token": API_KEY}
#         data = {
#             "keywords": text,
#             "sourceLanguage": "bo",
#             "targetLanguage": "zh-cn"
#         }
#         response = requests.post(API_URL, params=params, json=data, timeout=10)
#         response.raise_for_status()
#         result = response.json()
#
#         if result.get("code") == 1 and "data" in result and "text" in result["data"]:
#             return result["data"]["text"]
#         else:
#             print(f"âŒ ç¿»è¯‘å¤±è´¥ï¼š{text}ï¼Œé”™è¯¯ï¼š{result.get('info', 'æœªçŸ¥é”™è¯¯')}")
#             return ""
#     except Exception as e:
#         print(f"âŒ ç¿»è¯‘å¼‚å¸¸ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
#         raise  # æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘é‡è¯•
#
# # æ–‡ä»¶è·¯å¾„é…ç½®
# input_file = "task_MRC.json"
# output_file = "task_google_MRC.json"
#
# # è¯»å–åŸå§‹æ•°æ®
# try:
#     with open(input_file, "r", encoding="utf-8") as f:
#         original_data = json.load(f)
# except FileNotFoundError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
#     exit(1)
# except json.JSONDecodeError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {input_file} æ ¼å¼é”™è¯¯")
#     exit(1)
#
# # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶ç¡®å®šèµ·å§‹ç‚¹
# if os.path.exists(output_file):
#     try:
#         with open(output_file, "r", encoding="utf-8") as f:
#             processed_data = json.load(f)
#         start_index = len(processed_data)
#         print(f"â„¹ï¸ ç»§ç»­ä»ç¬¬ {start_index} æ¡å¼€å§‹ç¿»è¯‘")
#     except json.JSONDecodeError:
#         print(f"âŒ è¾“å‡ºæ–‡ä»¶ {output_file} æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°å¼€å§‹")
#         processed_data = []
#         start_index = 0
# else:
#     processed_data = []
#     start_index = 0
#
# # å¤„ç†å¹¶ç¿»è¯‘æ•°æ®
# for i in tqdm(range(start_index, len(original_data)), desc="ğŸ”„ ç¿»è¯‘inputå­—æ®µ", unit="æ¡"):
#     item = original_data[i]
#     if "input" not in item:
#         print(f"âš ï¸ ç¼ºå¤±inputå­—æ®µï¼š{item.get('id', 'æœªçŸ¥ID')}")
#         processed_data.append(item)
#         continue
#
#     # æ‰§è¡Œç¿»è¯‘
#     tibetan_input = translate_to_tibetan(item["input"])
#
#     # ä¿ç•™åŸå§‹æ•°æ®å¹¶æ·»åŠ ç¿»è¯‘ç»“æœ
#     new_item = item.copy()
#     new_item["input_zh"] = tibetan_input
#     processed_data.append(new_item)
#
#     # æ¯ç¿»è¯‘10æ¡ä¿å­˜ä¸€æ¬¡
#     if (i + 1) % 10 == 0:
#         try:
#             with open(output_file, "w", encoding="utf-8") as f:
#                 json.dump(processed_data, f, ensure_ascii=False, indent=4)
#             print(f"ğŸ’¾ å·²ä¿å­˜ {i + 1} æ¡æ•°æ®")
#         except Exception as e:
#             print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")
#             exit(1)
#
# # ç¿»è¯‘å®Œæˆåä¿å­˜æœ€ç»ˆç»“æœ
# try:
#     with open(output_file, "w", encoding="utf-8") as f:
#         json.dump(processed_data, f, ensure_ascii=False, indent=4)
#     print(f"\nâœ… ç¿»è¯‘å®Œæˆï¼å…±å¤„ç† {len(processed_data)} æ¡æ•°æ®")
#     print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")
# except Exception as e:
#     print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")
#     exit(1)




####ä¸‰ã€ä½¿ç”¨gpt-4oè¿›è¡Œè¯„ä¼°
import json
import openai
import os
import time
import unicodedata
import re
from tqdm import tqdm
from collections import Counter
from openai import OpenAI
from sentence_transformers import SentenceTransformer, util
import botok
from botok import WordTokenizer
from botok.config import Config
from pathlib import Path
import requests
from tenacity import retry, stop_after_attempt, wait_exponential

# Load LaBSE model for semantic similarity
labse_model = SentenceTransformer("/home/zhenmengyuan/.cache/modelscope/hub/models/sentence-transformers/LaBSE/")

# File paths
data_file = "task_google_MRC.json"
output_results_file = "google_deepseek_outputs.json"
eval_results_file = "google_deepseek_results.json"

# Load dataset
with open(data_file, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# Load existing results for checkpointing
if os.path.exists(output_results_file):
    with open(output_results_file, "r", encoding="utf-8") as f:
        existing_results = json.load(f)
else:
    existing_results = []

# Deduplicate: Skip already processed data
processed_inputs = {item["input_zh"] for item in existing_results}
dataset = [item for item in dataset if item["input_zh"] not in processed_inputs]

# OpenAI API client
client = OpenAI(base_url="http://chatapi.littlewheat.com/v1",
                api_key="sk-p4oPXDT8fchQaVROS85cRlsGy9vd8zq6511QfSSgzUVUiBPo")

# client = OpenAI(base_url = "https://ark.cn-beijing.volces.com/api/v3",
#                 api_key  = "aa1ce03a-6b61-46fc-b914-73fae89921f3")

# GPT-4o inference function
def get_openai_response(instruction, input_text, max_retries=3):
    for attempt in range(max_retries):
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–‡æœ¬é˜…è¯»ç†è§£çš„åŠ©æ‰‹"},
            {"role": "user", "content": f"{instruction}\n\n{input_text}"}
        ]
        try:
            response = client.chat.completions.create(
                model="deepseek-r1",
                messages=messages,
                temperature=0.6,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
            if attempt == max_retries - 1:
                return "ERROR"
            time.sleep(2)


# def get_doubao_response(instruction, input_text, max_retries=3):
#     for attempt in range(max_retries):
#         messages = [
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–‡æœ¬é˜…è¯»ç†è§£çš„åŠ©æ‰‹"},
#             {"role": "user", "content": f"{instruction}\n\n{input_text}"}
#         ]
#         try:
#             response = client.chat.completions.create(
#                 model="doubao-pro-32k-241215",
#                 messages=messages,
#                 temperature=0.6  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
#             )
#             return response.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
#             if attempt == max_retries - 1:
#                 return "ERROR"
#             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

# ### è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
def compute_semantic_similarity(pred, gold, model):
    embeddings = labse_model.encode([pred, gold], convert_to_tensor=True)
    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()
    return similarity_score

# Text cleaning and normalization
def clean_tibetan_text(text):
    text = unicodedata.normalize("NFC", text)
    text = re.sub(r"\s+", "", text)
    return text

# Tibetan syllable segmentation with botok
config = Config(dialect_name="general", base_path=Path.home())
tokenizer = WordTokenizer(config=config)

def tibetan_syllable_segment(text):
    tokens = tokenizer.tokenize(text, split_affixes=False)
    return " ".join([token.text for token in tokens])

# EM score computation
def compute_em(pred, gold):
    if not re.search(r"[à¼€-à¾¼0-9]", pred) or pred == "ERROR":
        return 0.0
    pred_clean = clean_tibetan_text(pred)
    gold_clean = clean_tibetan_text(gold)
    if pred_clean == gold_clean:
        return 1.0
    if pred_clean and gold_clean and (gold_clean in pred_clean or pred_clean in gold_clean):
        return 1.0
    similarity_score = compute_semantic_similarity(pred, gold, labse_model)
    if similarity_score > 0.9:
        return 1.0
    return 0.0

# F1 score computation
def compute_f1(pred, gold):
    pred_tokens = tibetan_syllable_segment(pred).split()
    gold_tokens = tibetan_syllable_segment(gold).split()
    pred_counter = Counter(pred_tokens)
    gold_counter = Counter(gold_tokens)
    common_tokens = pred_counter & gold_counter
    N_TP = sum(common_tokens.values())
    N_FP = sum(pred_counter.values()) - N_TP
    N_FN = sum(gold_counter.values()) - N_TP
    if N_TP == 0:
        return 0.0
    precision = N_TP / (N_TP + N_FP)
    recall = N_TP / (N_TP + N_FN)
    if precision + recall == 0:
        return 0.0
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

# Translation function
API_KEY = "ymg9YDPBCofY1DxJqxy6"
API_URL = "http://www.trans-home.com/api/index/translate"

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def translate_to_tibetan(text):
    if not text or not isinstance(text, str):
        return ""
    try:
        params = {"token": API_KEY}
        data = {
            "keywords": text,
            "sourceLanguage": "zh-cn",  # Fixed to translate from Chinese to Tibetan
            "targetLanguage": "bo"
        }
        response = requests.post(API_URL, params=params, json=data, timeout=10)
        response.raise_for_status()
        result = response.json()
        if result.get("code") == 1 and "data" in result and "text" in result["data"]:
            return result["data"]["text"]
        else:
            print(f"âŒ ç¿»è¯‘å¤±è´¥ï¼š{text}ï¼Œé”™è¯¯ï¼š{result.get('info', 'æœªçŸ¥é”™è¯¯')}")
            return ""
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¼‚å¸¸ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
        raise

# Main processing loop
results = existing_results
eval_scores = []

for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
    instruction = item["instruction"]
    input_zh = item["input_zh"]
    input_text = item["input"]
    gold_answer = item["output"]

    # Inference with GPT-4o
    output_zh = get_openai_response(instruction, input_zh)

    # Translate to Tibetan
    model_output = translate_to_tibetan(output_zh)

    # Compute evaluation metrics
    em_score = compute_em(model_output, gold_answer)
    f1_score = compute_f1(model_output, gold_answer)
    similarity = compute_semantic_similarity(model_output, gold_answer, labse_model)

    # Save results
    results.append({
        "instruction": instruction,
        "input": input_text,
        "input_zh": input_zh,
        "output_zh": output_zh,
        "model_output": model_output,
        "gold_answer": gold_answer,
        "similarity": similarity,
        "em": em_score,
        "f1": f1_score
    })

    eval_scores.append({
        "similarity": similarity,
        "em": em_score,
        "f1": f1_score
    })

    # Save every 10 items or at the end
    if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
        with open(output_results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")

# Compute average scores
eval_scores = [item for item in results if item["model_output"] != "ERROR"]
num_samples = len(eval_scores)
avg_scores = {
    "EM": sum(item["em"] for item in eval_scores) / num_samples if num_samples > 0 else 0.0,
    "F1": sum(item["f1"] for item in eval_scores) / num_samples if num_samples > 0 else 0.0
}

# Save evaluation results
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(avg_scores, f, ensure_ascii=False, indent=4)

print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")


