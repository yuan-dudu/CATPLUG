###ä¸€ã€å¤„ç†æ•°æ®
# import json
# import os
#
# # ğŸ“‚ Define file paths
# input_file = "/data/zhenmengyuan/LLaMA-Factory/data/tib_data/eval_MRC_zh2ti.json"
# output_dir = "/data/zhenmengyuan/LLaMA-Factory/saves/task_eval/MRC/"
# output_file = os.path.join(output_dir, "task_MRC.json")
#
# # ğŸ“‚ Ensure output directory exists
# os.makedirs(output_dir, exist_ok=True)
#
# # âœ… Read eval_MRC_zh2ti.json file
# with open(input_file, "r", encoding="utf-8") as f:
#     eval_data = json.load(f)
#
# # âœ… Convert format
# task_data = []
# for item in eval_data:
#     # Extract the Tibetan text from the history field (second element of the first history pair)
#     history_tibetan = item["history"][0][1] if item["history"] else ""
#     task_data.append({
#         "instruction": "è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å®Œæˆé˜…è¯»ç†è§£ä»»åŠ¡ï¼Œåªéœ€æ ¹æ®æœ€åçš„é—®é¢˜è¾“å‡ºç®€æ´çš„ç­”æ¡ˆã€‚",
#         "input": history_tibetan,
#         "output": item["output"]
#     })
#
# # âœ… Generate task_MRC.json
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(task_data, f, ensure_ascii=False, indent=4)
#
# # ğŸ“¢ Print completion message
# print(f"è½¬æ¢å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜è‡³: {output_file}")
#
# # # âœ… ç”Ÿæˆ `task_MRC_demo.json`ï¼ˆé€‰å–å‰ 10 æ¡ï¼‰
# # task_demo_data = task_data[:10]  # å–å‰ 10 æ¡
# # with open(demo_output_file, "w", encoding="utf-8") as f:
# #     json.dump(task_demo_data, f, ensure_ascii=False, indent=4)
#
# #print(f"Demo æ–‡ä»¶å·²ä¿å­˜è‡³: {demo_output_file}")

###äºŒã€åŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°
import json
import openai
import os
import time
import unicodedata
import re
from tqdm import tqdm
from collections import Counter
from openai import OpenAI # å¯¼å…¥OpenAI
from sentence_transformers import SentenceTransformer, util  # è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
import botok
from botok import WordTokenizer
from botok.config import Config
from pathlib import Path

# âœ… åŠ è½½ LaBSE æ¨¡å‹
labse_model = SentenceTransformer("/home/zhenmengyuan/.cache/modelscope/hub/models/sentence-transformers/LaBSE/")

# æ–‡ä»¶è·¯å¾„
data_file = "task_MRC.json"
output_results_file = "gpt-4o_outputs.json"
eval_results_file = "gpt-4o_results.json"

# åŠ è½½æ•°æ®é›†
with open(data_file, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# è¯»å–å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
if os.path.exists(output_results_file):
    with open(output_results_file, "r", encoding="utf-8") as f:
        existing_results = json.load(f)
else:
    existing_results = []

# **å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„æ•°æ®**
processed_inputs = {item["input"] for item in existing_results}
dataset = [item for item in dataset if item["input"] not in processed_inputs]

# âœ… OpenAI API å®¢æˆ·ç«¯ï¼ˆé€‚é…æ–°ç‰ˆ OpenAI SDKï¼‰
client = OpenAI(base_url = "http://chatapi.littlewheat.com/v1",
                api_key  = "sk-p4oPXDT8fchQaVROS85cRlsGy9vd8zq6511QfSSgzUVUiBPo")
# **OpenAI API è°ƒç”¨**
def get_gpt4o_response(instruction, input_text, max_retries=3):
    for attempt in range(max_retries):
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–‡æœ¬é˜…è¯»ç†è§£çš„åŠ©æ‰‹"},
            {"role": "user", "content": f"{instruction}\n\n{input_text}"}
        ]
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.8,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
            if attempt == max_retries - 1:
                return "ERROR"
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•

# âœ… è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
def compute_semantic_similarity(pred, gold, model):
    embeddings = labse_model.encode([pred, gold], convert_to_tensor=True)
    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()
    return similarity_score

# âœ… æ¸…ç† & è§„èŒƒåŒ–æ–‡æœ¬
def clean_tibetan_text(text):
    text = unicodedata.normalize("NFC", text)  # è§„èŒƒåŒ– Unicode
    text = re.sub(r"\s+", "", text)  # ä»…ç§»é™¤ç©ºæ ¼
    return text
# âœ… ä½¿ç”¨botokè¿›è¡Œåˆ†è¯
config = Config(dialect_name="general", base_path=Path.home())
tokenizer = WordTokenizer(config=config)
def tibetan_syllable_segment(text):
    """ä½¿ç”¨ Botok è¿›è¡Œè—æ–‡åˆ†è¯"""
    tokens = tokenizer.tokenize(text, split_affixes=False)
    return " ".join([token.text for token in tokens])  # è¿”å›ç©ºæ ¼åˆ†éš”çš„åˆ†è¯ç»“æœ

# âœ… è®¡ç®— EMï¼ˆå®Œå…¨åŒ¹é… + è¯­ä¹‰ç›¸ä¼¼åº¦ä¼˜åŒ–ï¼‰
def compute_em(pred, gold):
    # å¦‚æœé¢„æµ‹æ–‡æœ¬ä¸å«è—æ–‡å­—ç¬¦ï¼Œç›´æ¥åˆ¤å®šä¸ºä¸åŒ¹é…
    if not re.search(r"[à¼€-à¾¼0-9]", pred) or pred == "ERROR":
        return 0.0
    pred_clean = clean_tibetan_text(pred)
    gold_clean = clean_tibetan_text(gold)

    # 1ï¸âƒ£ **å®Œå…¨åŒ¹é…**
    if pred_clean == gold_clean:
        return 1.0

    # 2ï¸âƒ£ åŒ…å«å…³ç³»ï¼ˆéœ€ç¡®ä¿éƒ½ä¸ä¸ºç©ºï¼‰
    if pred_clean and gold_clean and (gold_clean in pred_clean or pred_clean in gold_clean):
        return 1.0

    # 3ï¸âƒ£ **è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦**
    similarity_score = compute_semantic_similarity(pred, gold, "labse")

    # 4ï¸âƒ£ **å¦‚æœè¯­ä¹‰ç›¸ä¼¼åº¦ > 0.95 ä¹Ÿç®—åŒ¹é…**
    if similarity_score > 0.9:
        return 1.0

    return 0.0  # âŒ å…¶ä»–æƒ…å†µä¸åŒ¹é…


# âœ… è®¡ç®— F1 Score
def compute_f1(pred, gold):
    pred_tokens = tibetan_syllable_segment(pred).split()
    gold_tokens = tibetan_syllable_segment(gold).split()

    pred_counter = Counter(pred_tokens)
    gold_counter = Counter(gold_tokens)

    # è®¡ç®— TPï¼ˆé¢„æµ‹æ­£ç¡®çš„è¯æ•°ï¼‰
    common_tokens = pred_counter & gold_counter
    N_TP = sum(common_tokens.values())

    # è®¡ç®— FPï¼ˆé”™è¯¯é¢„æµ‹çš„è¯æ•°ï¼‰
    N_FP = sum(pred_counter.values()) - N_TP

    # è®¡ç®— FNï¼ˆæ ‡å‡†ç­”æ¡ˆä¸­é—æ¼çš„è¯æ•°ï¼‰
    N_FN = sum(gold_counter.values()) - N_TP

    # é¿å…é™¤ä»¥é›¶
    if N_TP == 0:
        return 0.0

    # è®¡ç®— Precision å’Œ Recall
    precision = N_TP / (N_TP + N_FP)
    recall = N_TP / (N_TP + N_FN)
    if precision + recall == 0:
        return 0.0
    # è®¡ç®— F1 Score
    f1 = (2 * precision * recall) / (precision + recall)
    return f1


# ğŸš€ **è¿è¡Œæ¨¡å‹ & è¯„ä¼°**
results = existing_results  # åŠ è½½å·²å®Œæˆçš„ç»“æœ
eval_scores = []

# **è¿›åº¦æ¡**
for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
    instruction = item["instruction"]
    input_text = item["input"]
    gold_answer = item["output"]

    # **è°ƒç”¨ GPT-4o ç”Ÿæˆç­”æ¡ˆ**
    model_output = get_gpt4o_response(instruction, input_text)

    # **è®¡ç®—è¯„ä¼°æŒ‡æ ‡**
    em_score = compute_em(model_output, gold_answer)
    f1_score = compute_f1(model_output, gold_answer)
    similarity = compute_semantic_similarity(model_output, gold_answer, "labse")

    # **ä¿å­˜ç»“æœ**
    results.append({
        "instruction": instruction,
        "input": input_text,
        "gold_answer": gold_answer,
        "model_output": model_output,
        "similarity": similarity,
        "em": em_score,
        "f1": f1_score
    })

    eval_scores.append({
        "similarity": similarity,
        "em": em_score,
        "f1": f1_score
    })

    # **æ¯ 50 æ¡æ•°æ®å®æ—¶ä¿å­˜ä¸€æ¬¡**
    if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
        with open(output_results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")

# **è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡**
eval_scores = [item for item in results if item["model_output"] != "ERROR"]
num_samples = len(eval_scores)
avg_scores = {
    "EM": sum(item["em"] for item in eval_scores) / num_samples,
    "F1": sum(item["f1"] for item in eval_scores) / num_samples
}

# **ä¿å­˜è¯„ä¼°ç»“æœ**
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(avg_scores, f, ensure_ascii=False, indent=4)

print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")


# #####ä¸‰ã€å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œè°ƒæ•´è¯„ä¼°
# import json
#
# # æ–‡ä»¶è·¯å¾„
# input_file = "gpt-4o_outputs.json"
# output_file = "gpt-4o_results.json"
#
# # è¯»å– gpt-4o_outputs.json
# try:
#     with open(input_file, "r", encoding="utf-8") as f:
#         data = json.load(f)
# except FileNotFoundError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
#     exit(1)
# except json.JSONDecodeError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {input_file} æ ¼å¼é”™è¯¯")
#     exit(1)
#
# # æå– em å’Œ f1 åˆ†æ•°
# eval_scores = [{"em": item["em"], "f1": item["f1"]} for item in data]
#
# # è®¡ç®—æ•°æ®é‡å’Œå‡å€¼
# num_samples = len(eval_scores)
# avg_scores = {
#     "EM": sum(item["em"] for item in eval_scores) / num_samples if num_samples > 0 else 0.0,
#     "F1": sum(item["f1"] for item in eval_scores) / num_samples if num_samples > 0 else 0.0
# }
#
# # ä¿å­˜å¹³å‡è¯„ä¼°ç»“æœ
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(avg_scores, f, ensure_ascii=False, indent=4)
#
# print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {output_file}")


####å››ã€è¯•éªŒ
# pred = "à½à½¼à½„à¼‹à½¦à¾¤à¾±à½²à¼‹à½£à½¼1980à½£à½¼à½ à½²à¼‹à½Ÿà¾³9à½–à½ à½²à¼‹à½šà½ºà½¦12à½‰à½²à½“à¼‹à½§à¾²à½„à¼‹à½§à½ à½ºà¼‹à½à½´à½ à½ºà¼‹à½§à½´à½ à½ºà¼‹à½à½´à½£à¼‹à½‘à½´à¼‹à½¦à¾à¾±à½ºà½¦à¼‹à½˜à½¼à½‘à¼"
# gold = "à½¦à¾¤à¾±à½²à¼‹à½£à½¼1980à½£à½¼à½ à½²à¼‹à½Ÿà¾³9à½–à½ à½²à¼‹à½šà½ºà½¦12à½‰à½²à½“à¼"
# pred_clean=segment_tibetan_text(pred)
# gold_clean=segment_tibetan_text(gold)
# pred_clean1=segment_tibetan1_text(pred)
# gold_clean1=segment_tibetan1_text(gold)
#
# print(pred_clean)
# print(gold_clean)
# print(pred_clean1)
# print(gold_clean1)
#
# print(compute_f1(pred,gold))
# print(compute1_f1(pred,gold))
# print(compute_semantic_similarity(pred, gold,"sbert"))
# print(compute_semantic_similarity(pred, gold,"labse"))

###äº”ã€ç»Ÿè®¡æ— æ³•å›ç­”æ•°é‡
###ç»Ÿè®¡ä¸­æ–‡å›ç­”æ•°æ®
# import json
# import re
#
# # åŠ è½½ JSON æ–‡ä»¶
# with open('LCTW-gpt-4o_outputs.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # ç»Ÿè®¡åŒ…å«ä¸­æ–‡çš„ model_output æ•°æ®æ¡æ•°
# chinese_count = 0
#
# for item in data:
#     model_output = item['model_output']
#     # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ï¼ˆUnicode èŒƒå›´ \u4e00-\u9fffï¼‰
#     if re.search(r"[\u4e00-\u9fff]", model_output):
#         chinese_count += 1
#         print(model_output)
#
# # æ‰“å°ç»“æœ
# print(f"åŒ…å«ä¸­æ–‡çš„æ•°æ®æ¡æ•°: {chinese_count}")
