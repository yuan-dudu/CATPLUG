# ####ä¸€ã€å¤„ç†æ•°æ®
# import json
# import os
#
# # ğŸ“‚ å®šä¹‰æ–‡ä»¶è·¯å¾„
# input_file = "/data/zhenmengyuan/LLaMA-Factory/data/tib_data/eval_NER_zh2ti.json"
# output_dir = "/data/zhenmengyuan/LLaMA-Factory/saves/task_eval/NER/"
# output_file = os.path.join(output_dir, "task_NER.json")
# demo_output_file = os.path.join(output_dir, "task_NER_demo.json")  # ç”Ÿæˆ demo æ–‡ä»¶
#
# # ğŸ“‚ ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
# os.makedirs(output_dir, exist_ok=True)
#
# # âœ… è¯»å– eval_SUM.json æ–‡ä»¶
# with open(input_file, "r", encoding="utf-8") as f:
#     eval_data = json.load(f)
#
# # âœ… è½¬æ¢æ ¼å¼
# task_data = []
# for item in eval_data:
#     task_data.append({
#         "instruction": "è¯·å®Œæˆå‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼Œåªéœ€è¾“å‡ºä»¥ä¸‹æ–‡æœ¬ä¸­å­˜åœ¨çš„PERï¼ˆäººåï¼‰ã€LOCï¼ˆåœ°åï¼‰ã€ORGï¼ˆç»„ç»‡åï¼‰ï¼š",
#         "input": item["history"][0][1],
#         "output": item["output"]
#     })
# demo_data=task_data[:10]
# # âœ… ç”Ÿæˆ `task_SUM.json`
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(task_data, f, ensure_ascii=False, indent=4)
#
# with open(demo_output_file, "w", encoding="utf-8") as f:
#     json.dump(demo_data, f, ensure_ascii=False, indent=4)
# # ğŸ“¢ æ‰“å°å®Œæˆä¿¡æ¯
# print(f"è½¬æ¢å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜è‡³: {output_file}")


####å¤„ç†å®ä½“å•å¼•å·
# import json
# import re
#
#
# def remove_quotes_in_entities(input_file, output_file):
#     """
#     å¤„ç†NERä»»åŠ¡JSONæ–‡ä»¶ï¼Œå»é™¤outputå­—æ®µä¸­å®ä½“å€¼çš„å•å¼•å·
#     å‚æ•°ï¼š
#         input_file: åŸå§‹æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚"task_NER.json"ï¼‰
#         output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚"task_NER_processed.json"ï¼‰
#     """
#     # è¯»å–åŸå§‹æ–‡ä»¶
#     with open(input_file, 'r', encoding='utf-8') as f:
#         data = json.load(f)
#
#     # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆåŒ¹é… PER/LOC/ORG: åçš„å•å¼•å·å†…å®¹ï¼‰
#     pattern = re.compile(
#         r"(PER|LOC|ORG):\s*'(.*?)'",
#         flags=re.MULTILINE
#     )
#
#     # å¤„ç†æ¯ä¸ªæ¡ç›®
#     for item in data:
#         if "output" in item:
#             # æ›¿æ¢å•å¼•å·å¹¶ä¿ç•™å®ä½“ç»“æ„
#             processed = pattern.sub(r'\1: \2', item["output"])
#             item["output"] = processed
#
#     # ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
#     with open(output_file, 'w', encoding='utf-8') as f:
#         json.dump(data, f, ensure_ascii=False, indent=4)
#
#
# # ä½¿ç”¨ç¤ºä¾‹
# remove_quotes_in_entities(
#     input_file="task_NER_demo.json",
#     output_file="task_NER_demo_processed.json"
# )


#####æ¨¡å‹è¯„ä¼°
import os
import json
import openai
import time
import unicodedata
import re
import torch
from tqdm import tqdm
from rouge import Rouge
from collections import Counter
from openai import OpenAI # å¯¼å…¥OpenAI
from sacrebleu.metrics import BLEU
import botok
from botok import WordTokenizer
from botok.config import Config
from pathlib import Path

# æ–‡ä»¶è·¯å¾„
data_file = "task_NER.json"
output_results_file = "gpt-4o_outputs.json"
eval_results_file = "gpt-4o_results.json"

# åŠ è½½æ•°æ®é›†
with open(data_file, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# è¯»å–å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
if os.path.exists(output_results_file):
    with open(output_results_file, "r", encoding="utf-8") as f:
        existing_results = json.load(f)
else:
    existing_results = []

# **å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„æ•°æ®**
processed_inputs = {item["input"] for item in existing_results}
dataset = [item for item in dataset if item["input"] not in processed_inputs]

# âœ… OpenAI API å®¢æˆ·ç«¯ï¼ˆé€‚é…æ–°ç‰ˆ OpenAI SDKï¼‰
client = OpenAI(base_url = "http://chatapi.littlewheat.com/v1",
                api_key  = "sk-p4oPXDT8fchQaVROS85cRlsGy9vd8zq6511QfSSgzUVUiBPo")

str="(åªéœ€ä»¥PER: LOC: ORG: çš„æ ¼å¼è¾“å‡ºå­˜åœ¨çš„è—æ–‡å®ä½“ï¼Œä¸å­˜åœ¨çš„ç±»åˆ«ä¸ç”¨è¾“å‡º)"
# **OpenAI API è°ƒç”¨**
def get_gpt4o_response(instruction, input_text, max_retries=3):
    for attempt in range(max_retries):
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«çš„åŠ©æ‰‹ï¼Œä¸ç”¨åšå¤šä½™è§£é‡Š"},
            {"role": "user", "content": f"{instruction}+{str}\n\n{input_text}"}
        ]
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.8,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
            if attempt == max_retries - 1:
                return "ERROR"
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•


#æ–°å¢å®ä½“æå–å‡½æ•°
def extract_entities(text):
    """å¢å¼ºç‰ˆå®ä½“æå–å‡½æ•°ï¼ˆå¤„ç†None/æ— å®ä½“æ ‡è®°+å¤šå®ä½“åˆ†å‰²ï¼‰"""
    entities = {'PER': set(), 'LOC': set(), 'ORG': set()}

    # æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ”¯æŒå¤šæ ¼å¼ï¼‰
    pattern = r"""
        (?i)                    # å¿½ç•¥å¤§å°å†™
        (PER|LOC|ORG)           # å®ä½“ç±»å‹
        \s*:\s*                 # å†’å·å‰åå¯èƒ½æœ‰ç©ºæ ¼
        (                       # æ•è·å®ä½“å†…å®¹
            (?!                 # æ’é™¤ç‰¹å®šå¦å®šæƒ…å†µ
                None\b          # æ’é™¤çº¯None
                |æ— \b          # æ’é™¤ä¸­æ–‡æ— 
                |ï¼ˆæ²¡æœ‰ç»„ç»‡åï¼‰  # æ’é™¤ä¸­æ–‡æ‹¬å·æ ‡è®°
            )
            (?:                 # å†…å®¹åŒ¹é…ç»„
                '[^']*'         # å•å¼•å·å†…å®¹
                |               # æˆ–
                "[^"]*"         # åŒå¼•å·å†…å®¹
                |               # æˆ–
                [^\n:]+         # æ— å¼•å·å†…å®¹ï¼ˆç›´åˆ°æ¢è¡Œæˆ–å†’å·ï¼‰
            )
            (?:\s*,\s*[^\n:]+)* # å…è®¸é€—å·åˆ†éš”çš„å¤šå®ä½“
        )?
    """

    matches = re.findall(pattern, text, re.VERBOSE)

    for ent_type, ent_text in matches:
        ent_type = ent_type.upper()
        # å¤„ç†ç©ºå†…å®¹æˆ–ç‰¹æ®Šæ ‡è®°
        if not ent_text or ent_text.strip().lower() in ['none', 'æ— ', 'ï¼ˆæ²¡æœ‰ç»„ç»‡åï¼‰']:
            continue

        # æ¸…æ´—å†…å®¹ï¼šå»å¼•å·+å»ç©ºæ ¼+åˆ†å‰²å®ä½“
        cleaned = ent_text.strip().replace("'", "").replace('"', "").strip()
        parts = [p.strip() for p in re.split(r"\s*,\s*", cleaned) if p.strip()]

        if ent_type in entities and parts:
            entities[ent_type].update(parts)

    return entities

# æ–°å¢F1è®¡ç®—å‡½æ•°
def compute_entity_f1(gold_entities, pred_entities):
    """è®¡ç®—å®ä½“çº§åˆ«çš„F1åˆ†æ•°ï¼ˆå¤„ç†å…¨ç©ºæƒ…å†µï¼‰"""
    tp = {'PER': 0, 'LOC': 0, 'ORG': 0}
    fp = {'PER': 0, 'LOC': 0, 'ORG': 0}
    fn = {'PER': 0, 'LOC': 0, 'ORG': 0}

    # ç»Ÿè®¡æ¯ä¸ªç±»å‹çš„TP/FP/FN
    for ent_type in ['PER', 'LOC', 'ORG']:
        gold_set = gold_entities.get(ent_type, set())
        pred_set = pred_entities.get(ent_type, set())

        tp[ent_type] = len(gold_set & pred_set)
        fp[ent_type] = len(pred_set - gold_set)
        fn[ent_type] = len(gold_set - pred_set)

    # è®¡ç®—å®å¹³å‡F1ï¼ˆå¤„ç†å…¨ç©ºæƒ…å†µï¼‰
    f1_scores = []
    for ent_type in ['PER', 'LOC', 'ORG']:
        gold_has_ent = len(gold_entities.get(ent_type, set())) > 0
        pred_has_ent = len(pred_entities.get(ent_type, set())) > 0

        # å½“é»„é‡‘å’Œé¢„æµ‹éƒ½æ²¡æœ‰è¯¥ç±»å‹å®ä½“æ—¶ï¼Œè®°ä¸ºå®Œç¾åŒ¹é…
        if not gold_has_ent and not pred_has_ent:
            f1_scores.append(1.0)
            continue

        # æ­£å¸¸è®¡ç®—
        precision = tp[ent_type] / (tp[ent_type] + fp[ent_type]) if (tp[ent_type] + fp[ent_type]) > 0 else 0
        recall = tp[ent_type] / (tp[ent_type] + fn[ent_type]) if (tp[ent_type] + fn[ent_type]) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        f1_scores.append(f1)

    macro_f1 = sum(f1_scores) / len(f1_scores)
    return macro_f1


# ğŸš€ **è¿è¡Œæ¨¡å‹ & è¯„ä¼°**
results = existing_results  # åŠ è½½å·²å®Œæˆçš„ç»“æœ
eval_scores = []
total_tp = Counter()
total_fp = Counter()
total_fn = Counter()
# **è¿›åº¦æ¡**
for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
    instruction = item["instruction"]
    input_text = item["input"]
    gold_answer = item["output"]

    # **è°ƒç”¨ GPT-4o ç”Ÿæˆç­”æ¡ˆ**
    model_output = get_gpt4o_response(instruction, input_text)

    # æå–å®ä½“
    gold_entities = extract_entities(gold_answer)
    pred_entities = extract_entities(model_output)

    # è®¡ç®—å®ä½“F1
    macro_f1 = compute_entity_f1(gold_entities, pred_entities)


    # **ä¿å­˜ç»“æœ**
    results.append({
        "instruction": instruction,
        "input": input_text,
        "gold_answer": gold_answer,
        "model_output": model_output,
        "entity_f1": macro_f1
    })

    eval_scores.append({
        "entity_f1": macro_f1
    })

    # **æ¯ 50 æ¡æ•°æ®å®æ—¶ä¿å­˜ä¸€æ¬¡**
    if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
        with open(output_results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")

# **è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡**
eval_scores = [item for item in results if item["model_output"] != "ERROR"]
num_samples = len(eval_scores)
avg_scores = {
    "entity_f1": sum(item["entity_f1"] for item in eval_scores) / num_samples
}

# **ä¿å­˜è¯„ä¼°ç»“æœ**
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(avg_scores, f, ensure_ascii=False, indent=4)

print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")


###ç»Ÿè®¡ä¸­æ–‡è¾“å‡º
# import json
# import re
#
# # åŠ è½½ JSON æ–‡ä»¶
# with open('deepseek_outputs.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # ç»Ÿè®¡åŒ…å«ä¸­æ–‡çš„ model_output æ•°æ®æ¡æ•°
# chinese_count = 0
#
# for item in data:
#     model_output = item['model_output']
#     # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ï¼ˆUnicode èŒƒå›´ \u4e00-\u9fffï¼‰
#     if re.search(r"[\u4e00-\u9fff]", model_output):
#         chinese_count += 1
#         print(model_output)
#
# # æ‰“å°ç»“æœ
# print(f"åŒ…å«ä¸­æ–‡çš„æ•°æ®æ¡æ•°: {chinese_count}")



#####è°ƒæ•´è¾“å‡º
import os
import json
import re
import unicodedata
from collections import defaultdict

# æ–‡ä»¶è·¯å¾„
input_file="LCTW-deepseek_outputs.json"
#output_results_file = "deepseek_outputs.json"
eval_results_file = "LCTW-deepseek_results2.json"

# åŠ è½½å·²æœ‰çš„è¾“å‡ºç»“æœ
if os.path.exists(input_file):
    with open(input_file, "r", encoding="utf-8") as f:
        results = json.load(f)
else:
    raise FileNotFoundError(f"æ–‡ä»¶ {input_file} ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿å·²ç”Ÿæˆè¯¥æ–‡ä»¶ã€‚")


# å®ä½“æå–å‡½æ•°

def extract_entities(text):
    """æ”¹è¿›ç‰ˆå®ä½“æå–å‡½æ•°ï¼Œæ­£ç¡®å¤„ç†ç©ºæ ¼åˆ†éš”çš„å®ä½“ç±»å‹"""
    entities = {'PER': set(), 'LOC': set(), 'ORG': set()}

    # æ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ•è·å®ä½“å†…å®¹ç›´åˆ°ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–å­—ç¬¦ä¸²æœ«å°¾
    pattern = r"""
        (?i)                    # å¿½ç•¥å¤§å°å†™
        \b(PER|LOC|ORG)\b       # å®ä½“ç±»å‹ä½œä¸ºç‹¬ç«‹å•è¯
        \s*:\s*                 # å†’å·å‰åå¯èƒ½æœ‰ç©ºæ ¼
        (                       # æ•è·å®ä½“å†…å®¹
            (?:                 # éæ•è·ç»„ï¼Œç¡®ä¿ä¸è·¨å®ä½“ç±»å‹
                (?!\b(?:PER|LOC|ORG)\b\s*:)  # è´Ÿå‘å‰ç»ï¼Œæ’é™¤ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹
                .               # åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
            )*?                 # éè´ªå©ªåŒ¹é…ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–æœ«å°¾
        )
        (?=\s*\b(?:PER|LOC|ORG)\b\s*:|$|\s*$)  # æ­£å‘æ–­è¨€ï¼Œç¡®ä¿åœåœ¨ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–å­—ç¬¦ä¸²æœ«å°¾
    """

    matches = re.findall(pattern, text, re.VERBOSE)

    for ent_type, ent_text in matches:
        ent_type = ent_type.upper()
        if not ent_text or ent_text.strip().lower() in ['none', 'æ— ', 'ï¼ˆæ²¡æœ‰ç»„ç»‡åï¼‰']:
            continue

        # æ¸…æ´—å†…å®¹ï¼šå»å¼•å·ã€å½’ä¸€åŒ–ã€å»ç©ºæ ¼
        cleaned = ent_text.strip().replace("'", "").replace('"', '')
        cleaned = unicodedata.normalize('NFC', cleaned)
        cleaned = re.sub(r'\s+', '', cleaned)  # å»é™¤æ‰€æœ‰ç©ºæ ¼

        # ä½¿ç”¨è—æ–‡é€—å·å’Œä¸­è‹±æ–‡é€—å·åˆ†å‰²
        parts = [p for p in re.split(r'[,\u0f0d]', cleaned) if p]

        # è¿‡æ»¤æ‰å¯èƒ½è¯¯åŒ¹é…çš„å®ä½“ç±»å‹å…³é”®è¯
        filtered_parts = []
        for part in parts:
            if part.strip().upper() not in {'PER', 'LOC', 'ORG'}:
                filtered_parts.append(part)

        if ent_type in entities and filtered_parts:
            entities[ent_type].update(filtered_parts)

    return entities


def compute_entity_f1(gold_entities, pred_entities):
    """
    æ”¹è¿›ç‰ˆF1è®¡ç®—ï¼Œç²¾ç¡®å¤„ç†ç©ºå€¼æƒ…å†µï¼š
    1. å½“ä¸”ä»…å½“é»„é‡‘æ ‡å‡†æˆ–é¢„æµ‹æ ‡å‡†ä¸­æŸç±»å‹å­˜åœ¨å®ä½“æ—¶æ‰è®¡ç®—è¯¥ç±»å‹
    2. å½“ä¸¤è€…éƒ½ä¸ºç©ºæ—¶ï¼Œè¯¥ç±»å‹ä¸å‚ä¸è®¡ç®—
    3. å®å¹³å‡åªè®¡ç®—å®é™…å‚ä¸çš„ç±»å‹
    """
    f1_scores = []
    entity_types = ['PER', 'LOC', 'ORG']

    for ent_type in entity_types:
        gold_set = gold_entities.get(ent_type, set())
        pred_set = pred_entities.get(ent_type, set())

        # åˆ¤æ–­æ˜¯å¦éœ€è¦è®¡ç®—è¯¥ç±»å‹
        gold_has_entities = len(gold_set) > 0
        pred_has_entities = len(pred_set) > 0

        if not gold_has_entities and not pred_has_entities:
            continue  # åŒæ–¹éƒ½ä¸ºç©ºæ—¶ä¸å‚ä¸è®¡ç®—

        tp = len(gold_set & pred_set)
        fp = len(pred_set - gold_set)
        fn = len(gold_set - pred_set)

        # å¤„ç†åˆ†æ¯ä¸ºé›¶çš„æƒ…å†µ
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        f1_scores.append(f1)

    # å½“æ‰€æœ‰ç±»å‹éƒ½ä¸å‚ä¸è®¡ç®—æ—¶è¿”å›1
    return sum(f1_scores) / len(f1_scores) if f1_scores else 1.0


# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
eval_scores = []
for item in results:
    gold_answer = item["gold_answer"]
    model_output = item["model_output"]

    gold_entities = extract_entities(gold_answer)
    pred_entities = extract_entities(model_output)

    macro_f1 = compute_entity_f1(gold_entities, pred_entities)

    item["entity_f1"] = macro_f1
    eval_scores.append({"entity_f1": macro_f1})

# è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡
num_samples = len(results)
if num_samples > 0:
    avg_scores = {
        "entity_f1": sum(item["entity_f1"] for item in results) / num_samples
    }
else:
    avg_scores = {"entity_f1": 0.0}

# ä¿å­˜ç»“æœ
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(avg_scores, f, ensure_ascii=False, indent=4)


print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")