###ç¿»è¯‘æ•°æ®
# import json
# import os
# from openai import OpenAI
# from tqdm import tqdm

# # Data paths
# data_path = '/mnt/data/zhenmengyuan/LLaMA-Factory/saves/task_eval/ti-zh/NER/task_NER.json'
# output_file = '/mnt/data/zhenmengyuan/LLaMA-Factory/saves/task_eval/ti-zh/NER/task_nohistory_NER.json'


# # Load dataset
# try:
#     with open(data_path, 'r', encoding='utf-8') as f:
#         dataset = json.load(f)
# except FileNotFoundError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_path} ä¸å­˜åœ¨")
#     exit(1)
# except json.JSONDecodeError:
#     print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_path} æ ¼å¼é”™è¯¯")
#     exit(1)

# # Load existing translated results (if any)
# if os.path.exists(output_file):
#     try:
#         with open(output_file, 'r', encoding='utf-8') as f:
#             translated_data = json.load(f)
#         print(f"â„¹ï¸ ç»§ç»­ä»ç¬¬ {len(translated_data)} æ¡å¼€å§‹ç¿»è¯‘")
#     except json.JSONDecodeError:
#         print(f"âŒ è¾“å‡ºæ–‡ä»¶ {output_file} æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°å¼€å§‹")
#         translated_data = []
# else:
#     translated_data = []

# # Determine the number of already translated items
# translated_count = len(translated_data)

# # Initialize OpenAI client
# client = OpenAI(api_key="0", base_url="http://0.0.0.0:8001/v1")

# # Translation function
# def translate(system_content, instruction, input_text):
#     messages = [
#         {"role": "system", "content": system_content},
#         {"role": "user", "content": f"{instruction}\n\n{input_text}"}
#     ]
#     try:
#         result = client.chat.completions.create(
#             messages=messages,
#             model="saves/qwen2.5/ti-zh/qwen-ti",
#             temperature=0.6,
#             top_p=0.6,
#         )
#         return result.choices[0].message.content.strip()
#     except Exception as e:
#         print(f"âŒ ç¿»è¯‘å¤±è´¥ï¼š{input_text}ï¼Œé”™è¯¯ï¼š{str(e)}")
#         return ""

# # Translation process
# SAVE_INTERVAL = 20
# system_content = "You are a professional translator fluent in both Chinese and Tibetan."
# instruction = "Translate the following sentences from Tibetan to Chinese."

# for i, item in enumerate(tqdm(dataset[translated_count:], desc="ğŸ”„ ç¿»è¯‘inputå­—æ®µ", unit="æ¡", initial=translated_count, total=len(dataset))):
#     input_text = item.get('input', '')
#     if not input_text:
#         print(f"âš ï¸ ç¼ºå¤±inputå­—æ®µï¼š{item.get('id', 'æœªçŸ¥ID')}")
#         translated_data.append(item)
#         continue

#     # Translate Tibetan input to Chinese
#     input_zh = translate(system_content, instruction, input_text)

#     # Retain original data and add translated input_zh
#     new_item = item.copy()
#     new_item['input_zh'] = input_zh
#     translated_data.append(new_item)

#     # Display progress
#     print(f"Translated {i + 1}/{len(dataset)}: {input_zh}")

#     # Save every SAVE_INTERVAL items or at the end
#     if (i + 1) % SAVE_INTERVAL == 0 or (i + 1) == len(dataset):
#         try:
#             with open(output_file, 'w', encoding='utf-8') as f:
#                 json.dump(translated_data, f, ensure_ascii=False, indent=4)
#             print(f"ğŸ’¾ å·²ä¿å­˜ {i + 1} æ¡æ•°æ®")
#         except Exception as e:
#             print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")
#             exit(1)

# # Save final results
# try:
#     with open(output_file, 'w', encoding='utf-8') as f:
#         json.dump(translated_data, f, ensure_ascii=False, indent=4)
#     print(f"\nâœ… ç¿»è¯‘å®Œæˆï¼å…±å¤„ç† {len(translated_data)} æ¡æ•°æ®")
#     print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š{output_file}")
# except Exception as e:
#     print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")
#     exit(1)




####æ¨¡å‹è¯„ä¼°
import json
import os
from openai import OpenAI
from transformers.generation.utils import GenerationMixin
import re
import unicodedata
from collections import Counter
from tqdm import tqdm
from pathlib import Path
import time
import torch
from rouge import Rouge
from sacrebleu.metrics import BLEU


# File paths
data_file = "task_nohistory_NER.json"
output_results_file = "nohistory-doubao_outputs.json"
eval_results_file = "nohistory-doubao_results.json"

# Load dataset
try:
    with open(data_file, "r", encoding="utf-8") as f:
        dataset = json.load(f)
except FileNotFoundError:
    print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_file} ä¸å­˜åœ¨")
    exit(1)
except json.JSONDecodeError:
    print(f"âŒ è¾“å…¥æ–‡ä»¶ {data_file} æ ¼å¼é”™è¯¯")
    exit(1)

# Load existing results (for checkpointing)
if os.path.exists(output_results_file):
    with open(output_results_file, "r", encoding="utf-8") as f:
        existing_results = json.load(f)
else:
    existing_results = []

# Deduplicate: Skip already processed data
processed_inputs = {item["input_zh"] for item in existing_results if "input_zh" in item}
dataset = [item for item in dataset if "input_zh" in item and item["input_zh"] not in processed_inputs]

# Initialize translation client
translate_client = OpenAI(api_key="0", base_url="http://0.0.0.0:8001/v1")

# Initialize inference client
# inference_client = OpenAI(base_url="http://chatapi.littlewheat.com/v1",
#                           api_key="sk-eU6vJLbjn2cznR4sK1lV7f9zDSFARcxcbihirVfM2wOd2fiS")

client = OpenAI(base_url = "https://ark.cn-beijing.volces.com/api/v3",
                api_key  = "30f8d6d0-96d1-4334-8244-eb15bc951063")

# Translation function
def translate(system_content, instruction, input_text):
    messages = [
        {"role": "system", "content": system_content},
    ]
    messages.append({"role": "user", "content": f"{instruction}\n\n{input_text}"})
    try:
        result = translate_client.chat.completions.create(
            messages=messages,
            model="saves/qwen2.5/ti-zh/qwen-ti",
            temperature=0.6,
            top_p=0.6,
        )
        return result.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ ç¿»è¯‘å¤±è´¥ï¼š{input_text}ï¼Œé”™è¯¯ï¼š{str(e)}")
        return "ERROR"
    
str="(åªéœ€ä»¥PER: LOC: ORG: çš„æ ¼å¼è¾“å‡ºå­˜åœ¨çš„å®ä½“ï¼Œä¸å­˜åœ¨çš„ç±»åˆ«ä¸ç”¨è¾“å‡º)"
# Inference function with retry mechanism
# def get_openai_response(instruction, input_text, max_retries=3):
#     for attempt in range(max_retries):
#         messages = [
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«çš„åŠ©æ‰‹ï¼Œä¸ç”¨åšå¤šä½™è§£é‡Š"},
#             {"role": "user", "content": f"{instruction}+{str}\n\n{input_text}"}
#         ]
#         try:
#             response = inference_client.chat.completions.create(
#                 model="gpt-4o",
#                 messages=messages,
#                 temperature=0.6,
#             )
#             return response.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
#             if attempt == max_retries - 1:
#                 return "ERROR"
#             time.sleep(2)

def get_doubao_response(instruction, input_text, max_retries=3):
    for attempt in range(max_retries):
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«çš„åŠ©æ‰‹ï¼Œä¸ç”¨åšå¤šä½™è§£é‡Š"},
            {"role": "user", "content": f"{instruction}+{str}\n\n{input_text}"}
        ]
        try:
            response = client.chat.completions.create(
                model="doubao-pro-32k-241215",
                messages=messages,
                temperature=0.6  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
            if attempt == max_retries - 1:
                return "ERROR"
            time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•


#æ–°å¢å®ä½“æå–å‡½æ•°
def extract_entities(text):
    """æ”¹è¿›ç‰ˆå®ä½“æå–å‡½æ•°ï¼Œæ­£ç¡®å¤„ç†ç©ºæ ¼åˆ†éš”çš„å®ä½“ç±»å‹"""
    entities = {'PER': set(), 'LOC': set(), 'ORG': set()}

    # æ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼ï¼Œæ•è·å®ä½“å†…å®¹ç›´åˆ°ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–å­—ç¬¦ä¸²æœ«å°¾
    pattern = r"""
        (?i)                    # å¿½ç•¥å¤§å°å†™
        \b(PER|LOC|ORG)\b       # å®ä½“ç±»å‹ä½œä¸ºç‹¬ç«‹å•è¯
        \s*:\s*                 # å†’å·å‰åå¯èƒ½æœ‰ç©ºæ ¼
        (                       # æ•è·å®ä½“å†…å®¹
            (?:                 # éæ•è·ç»„ï¼Œç¡®ä¿ä¸è·¨å®ä½“ç±»å‹
                (?!\b(?:PER|LOC|ORG)\b\s*:)  # è´Ÿå‘å‰ç»ï¼Œæ’é™¤ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹
                .               # åŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰
            )*?                 # éè´ªå©ªåŒ¹é…ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–æœ«å°¾
        )
        (?=\s*\b(?:PER|LOC|ORG)\b\s*:|$|\s*$)  # æ­£å‘æ–­è¨€ï¼Œç¡®ä¿åœåœ¨ä¸‹ä¸€ä¸ªå®ä½“ç±»å‹æˆ–å­—ç¬¦ä¸²æœ«å°¾
    """

    matches = re.findall(pattern, text, re.VERBOSE)

    for ent_type, ent_text in matches:
        ent_type = ent_type.upper()
        if not ent_text or ent_text.strip().lower() in ['none', 'æ— ', 'ï¼ˆæ²¡æœ‰ç»„ç»‡åï¼‰']:
            continue

        # æ¸…æ´—å†…å®¹ï¼šå»å¼•å·ã€å½’ä¸€åŒ–ã€å»ç©ºæ ¼
        cleaned = ent_text.strip().replace("'", "").replace('"', '')
        cleaned = unicodedata.normalize('NFC', cleaned)
        cleaned = re.sub(r'\s+', '', cleaned)  # å»é™¤æ‰€æœ‰ç©ºæ ¼

        # ä½¿ç”¨è—æ–‡é€—å·å’Œä¸­è‹±æ–‡é€—å·åˆ†å‰²
        parts = [p for p in re.split(r'[,\u0f0d]', cleaned) if p]

        # è¿‡æ»¤æ‰å¯èƒ½è¯¯åŒ¹é…çš„å®ä½“ç±»å‹å…³é”®è¯
        filtered_parts = []
        for part in parts:
            if part.strip().upper() not in {'PER', 'LOC', 'ORG'}:
                filtered_parts.append(part)

        if ent_type in entities and filtered_parts:
            entities[ent_type].update(filtered_parts)

    return entities


def compute_entity_f1(gold_entities, pred_entities):
    """
    æ”¹è¿›ç‰ˆF1è®¡ç®—ï¼Œç²¾ç¡®å¤„ç†ç©ºå€¼æƒ…å†µï¼š
    1. å½“ä¸”ä»…å½“é»„é‡‘æ ‡å‡†æˆ–é¢„æµ‹æ ‡å‡†ä¸­æŸç±»å‹å­˜åœ¨å®ä½“æ—¶æ‰è®¡ç®—è¯¥ç±»å‹
    2. å½“ä¸¤è€…éƒ½ä¸ºç©ºæ—¶ï¼Œè¯¥ç±»å‹ä¸å‚ä¸è®¡ç®—
    3. å®å¹³å‡åªè®¡ç®—å®é™…å‚ä¸çš„ç±»å‹
    """
    f1_scores = []
    entity_types = ['PER', 'LOC', 'ORG']

    for ent_type in entity_types:
        gold_set = gold_entities.get(ent_type, set())
        pred_set = pred_entities.get(ent_type, set())

        # åˆ¤æ–­æ˜¯å¦éœ€è¦è®¡ç®—è¯¥ç±»å‹
        gold_has_entities = len(gold_set) > 0
        pred_has_entities = len(pred_set) > 0

        if not gold_has_entities and not pred_has_entities:
            continue  # åŒæ–¹éƒ½ä¸ºç©ºæ—¶ä¸å‚ä¸è®¡ç®—

        tp = len(gold_set & pred_set)
        fp = len(pred_set - gold_set)
        fn = len(gold_set - pred_set)

        # å¤„ç†åˆ†æ¯ä¸ºé›¶çš„æƒ…å†µ
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        f1_scores.append(f1)

    # å½“æ‰€æœ‰ç±»å‹éƒ½ä¸å‚ä¸è®¡ç®—æ—¶è¿”å›1
    return sum(f1_scores) / len(f1_scores) if f1_scores else 1.0



# Main processing loop
results = existing_results
eval_scores = []

# System prompt and translation instructions
system_content = "You are a professional translator fluent in both Chinese and Tibetan."
instruction_ti = "Translate the following sentences from Chinese to Tibetan. "

for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
    instruction = item["instruction"]
    input_zh = item["input_zh"]
    input_text = item["input"]  # Original Tibetan input for history
    gold_answer = item["output"]

    # 1. Use GPT-4o for inference on instruction + input_zh
    output_zh = get_doubao_response(instruction, input_zh)

    # 2. Translate output_zh back to Tibetan with history
    model_output = translate(system_content, instruction_ti, output_zh)

    # 3. Compute evaluation metrics
    # æå–å®ä½“
    gold_entities = extract_entities(gold_answer)
    pred_entities = extract_entities(model_output)

    # è®¡ç®—å®ä½“F1
    macro_f1 = compute_entity_f1(gold_entities, pred_entities)

    # Save results
    results.append({
        "instruction": instruction,
        "input": input_text,
        "input_zh": input_zh,
        "output_zh": output_zh,
        "model_output": model_output,
        "gold_answer": gold_answer,
        "entity_f1": macro_f1

    })

    # Save every 10 items or at the end
    if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
        try:
            with open(output_results_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            print(f"âœ… {i + 1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{str(e)}")


# Compute average evaluation scores
eval_scores = [item for item in results if item["model_output"] != "ERROR"]
num_samples = len(eval_scores)
if num_samples > 0:
    avg_scores = {
        "entity_f1": sum(item["entity_f1"] for item in eval_scores) / num_samples
    }
else:
    avg_scores = {"entity_f1": 0.0}

# Save evaluation results
try:
    with open(eval_results_file, "w", encoding="utf-8") as f:
        json.dump(avg_scores, f, ensure_ascii=False, indent=4)
    print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")
except Exception as e:
    print(f"âŒ è¯„ä¼°ç»“æœä¿å­˜å¤±è´¥ï¼š{str(e)}")

