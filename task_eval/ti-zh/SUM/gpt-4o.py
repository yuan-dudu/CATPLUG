# ####ä¸€ã€å¤„ç†æ•°æ®
# import json
# import os
#
# # ğŸ“‚ å®šä¹‰æ–‡ä»¶è·¯å¾„
# input_file = "/data/zhenmengyuan/LLaMA-Factory/data/tib_data/eval_sum.json"
# output_dir = "/data/zhenmengyuan/LLaMA-Factory/saves/task_eval/SUM/"
# output_file = os.path.join(output_dir, "task_SUM.json")
# #demo_output_file = os.path.join(output_dir, "task_SUM_demo.json")  # ç”Ÿæˆ demo æ–‡ä»¶
#
# # ğŸ“‚ ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
# os.makedirs(output_dir, exist_ok=True)
#
# # âœ… è¯»å– eval_SUM.json æ–‡ä»¶
# with open(input_file, "r", encoding="utf-8") as f:
#     eval_data = json.load(f)
#
# # âœ… è½¬æ¢æ ¼å¼
# task_data = []
# for item in eval_data:
#     task_data.append({
#         "instruction": "è¯·æ ¹æ®ä»¥ä¸‹æ–‡æœ¬å®Œæˆæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ï¼Œä¸éœ€è¦åšå¤šä½™è§£é‡Šã€‚",
#         "input": item["content_tibetan"],
#         "output": item["summary_tibetan"]
#     })
#
# # âœ… ç”Ÿæˆ `task_SUM.json`
# with open(output_file, "w", encoding="utf-8") as f:
#     json.dump(task_data, f, ensure_ascii=False, indent=4)
#
# # ğŸ“¢ æ‰“å°å®Œæˆä¿¡æ¯
# print(f"è½¬æ¢å®Œæˆï¼Œæ–‡ä»¶å·²ä¿å­˜è‡³: {output_file}")



####äºŒã€è°ƒç”¨æ¨¡å‹è¿›è¡Œè¯„ä¼°
# import os
# import json
# import openai
# import time
# import unicodedata
# import re
# import torch
# from tqdm import tqdm
# from rouge import Rouge
# from collections import Counter
# from openai import OpenAI # å¯¼å…¥OpenAI
# from sacrebleu.metrics import BLEU
# import botok
# from botok import WordTokenizer
# from botok.config import Config
# from pathlib import Path
#
# # æ–‡ä»¶è·¯å¾„
# data_file = "task_SUM.json"
# output_results_file = "gpt-4o_outputs.json"
# eval_results_file = "gpt-4o_results.json"
#
# # åŠ è½½æ•°æ®é›†
# with open(data_file, "r", encoding="utf-8") as f:
#     dataset = json.load(f)
#
# # è¯»å–å·²æœ‰ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰
# if os.path.exists(output_results_file):
#     with open(output_results_file, "r", encoding="utf-8") as f:
#         existing_results = json.load(f)
# else:
#     existing_results = []
#
# # **å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„æ•°æ®**
# processed_inputs = {item["input"] for item in existing_results}
# dataset = [item for item in dataset if item["input"] not in processed_inputs]
#
# # âœ… OpenAI API å®¢æˆ·ç«¯ï¼ˆé€‚é…æ–°ç‰ˆ OpenAI SDKï¼‰
# client = OpenAI(base_url = "http://chatapi.littlewheat.com/v1",
#                 api_key  = "sk-p4oPXDT8fchQaVROS85cRlsGy9vd8zq6511QfSSgzUVUiBPo")
# # **OpenAI API è°ƒç”¨**
# def get_gpt4o_response(instruction, input_text, max_retries=3):
#     for attempt in range(max_retries):
#         messages = [
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿æ–‡æœ¬æ‘˜è¦çš„åŠ©æ‰‹"},
#             {"role": "user", "content": f"{instruction}\n\n{input_text}"}
#         ]
#         try:
#             response = client.chat.completions.create(
#                 model="gpt-4o",
#                 messages=messages,
#                 temperature=0.8,  # ä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šå›ç­”
#             )
#             return response.choices[0].message.content.strip()
#         except Exception as e:
#             print(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰ï¼š{e}")
#             if attempt == max_retries - 1:
#                 return "ERROR"
#             time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
#
#
# # âœ… ä½¿ç”¨botokè¿›è¡Œåˆ†è¯
# config = Config(dialect_name="general", base_path=Path.home())
# tokenizer = WordTokenizer(config=config)
# def segment_tibetan_text(text):
#     """ä½¿ç”¨ Botok è¿›è¡Œè—æ–‡åˆ†è¯"""
#     tokens = tokenizer.tokenize(text, split_affixes=False)
#     return " ".join([token.text for token in tokens])  # è¿”å›ç©ºæ ¼åˆ†éš”çš„åˆ†è¯ç»“æœ
#
# # âœ… è®¡ç®— BLEU
# def compute_bleu(pred: str, gold: str) -> float:
#     """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""
#
#     # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
#     bleu = BLEU(
#         smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
#         smooth_value=1,  # add-k çš„ k å€¼
#         max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
#         effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
#         tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
#         lowercase=False  # ä¸è½¬å°å†™
#     )
#
#     # âœ… è—æ–‡åˆ†è¯ï¼ˆå‡è®¾ tibetan_syllable_segment å·²æ­£ç¡®å®šä¹‰ï¼‰
#     pred_tok = segment_tibetan_text(pred)
#     gold_tok = segment_tibetan_text(gold)
#
#     # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
#     # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
#     bleu_score = bleu.sentence_score(pred_tok, [gold_tok])
#
#     return bleu_score.score
# # âœ… è®¡ç®— ROUGE
# rouge_evaluator = Rouge()
# def compute_rouge(pred, gold):
#     # ç©ºå€¼è¿‡æ»¤
#     pred = str(pred).strip() or " "  # ç©ºæ–‡æœ¬æ›¿æ¢ä¸ºå•ç©ºæ ¼
#     gold = str(gold).strip() or " "
#
#     pred_clean = segment_tibetan_text(pred)#éœ€è¦åˆ†è¯
#     gold_clean = segment_tibetan_text(gold)
#     # äºŒæ¬¡éªŒè¯
#     if not pred_clean.strip() or not gold_clean.strip():
#         return {
#             "rouge-1": {"f": 0.0},
#             "rouge-2": {"f": 0.0},
#             "rouge-l": {"f": 0.0}
#         }
#
#     try:
#         scores = rouge_evaluator.get_scores(pred_clean, gold_clean)
#         return scores[0]
#     except Exception as e:
#         print(f"âš ï¸ ROUGE è®¡ç®—å¤±è´¥: pred='{pred_clean}' gold='{gold_clean}'")
#         return {
#             "rouge-1": {"f": 0.0},
#             "rouge-2": {"f": 0.0},
#             "rouge-l": {"f": 0.0}
#         }
#
# # ğŸš€ **è¿è¡Œæ¨¡å‹ & è¯„ä¼°**
# results = existing_results  # åŠ è½½å·²å®Œæˆçš„ç»“æœ
# eval_scores = []
#
# # **è¿›åº¦æ¡**
# for i, item in enumerate(tqdm(dataset, desc="ğŸš€ å¤„ç†æ•°æ®", unit="æ¡")):
#     instruction = item["instruction"]
#     input_text = item["input"]
#     gold_answer = item["output"]
#
#     # **è°ƒç”¨ GPT-4o ç”Ÿæˆç­”æ¡ˆ**
#     model_output = get_gpt4o_response(instruction, input_text)
#
#     # **è®¡ç®—è¯„ä¼°æŒ‡æ ‡**
#     rouge_scores = compute_rouge(model_output, gold_answer)
#     bleu_scores = compute_bleu(model_output, gold_answer)
#
#
#     # **ä¿å­˜ç»“æœ**
#     results.append({
#         "instruction": instruction,
#         "input": input_text,
#         "gold_answer": gold_answer,
#         "model_output": model_output,
#         "rouge-1": rouge_scores["rouge-1"]["f"],
#         "rouge-2": rouge_scores["rouge-2"]["f"],
#         "rouge-l": rouge_scores["rouge-l"]["f"],
#         "bleu": bleu_scores
#     })
#
#     eval_scores.append({
#         "rouge-1": rouge_scores["rouge-1"]["f"],
#         "rouge-2": rouge_scores["rouge-2"]["f"],
#         "rouge-l": rouge_scores["rouge-l"]["f"],
#         "bleu": bleu_scores
#     })
#
#     # **æ¯ 50 æ¡æ•°æ®å®æ—¶ä¿å­˜ä¸€æ¬¡**
#     if (i + 1) % 10 == 0 or (i + 1) == len(dataset):
#         with open(output_results_file, "w", encoding="utf-8") as f:
#             json.dump(results, f, ensure_ascii=False, indent=4)
#         print(f"âœ… {i+1}/{len(dataset)} æ¡æ•°æ®å·²ä¿å­˜è‡³ {output_results_file}")
#
# # **è®¡ç®—å¹³å‡è¯„ä¼°æŒ‡æ ‡**
# eval_scores = [item for item in results if item["model_output"] != "ERROR"]
# num_samples = len(eval_scores)
# avg_scores = {
#     "ROUGE-1": sum(item["rouge-1"] for item in eval_scores) / num_samples,
#     "ROUGE-2": sum(item["rouge-2"] for item in eval_scores) / num_samples,
#     "ROUGE-L": sum(item["rouge-l"] for item in eval_scores) / num_samples,
#     "BLEU": sum(item["bleu"] for item in eval_scores) / num_samples
# }
#
# # **ä¿å­˜è¯„ä¼°ç»“æœ**
# with open(eval_results_file, "w", encoding="utf-8") as f:
#     json.dump(avg_scores, f, ensure_ascii=False, indent=4)
#
# print(f"ğŸ“Š è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")


###ç»Ÿè®¡ä¸­æ–‡è¾“å‡º
# import json
# import re
#
# # åŠ è½½ JSON æ–‡ä»¶
# with open('deepseek_outputs.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
#
# # ç»Ÿè®¡åŒ…å«ä¸­æ–‡çš„ model_output æ•°æ®æ¡æ•°
# chinese_count = 0
#
# for item in data:
#     model_output = item['model_output']
#     # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ï¼ˆUnicode èŒƒå›´ \u4e00-\u9fffï¼‰
#     if re.search(r"[\u4e00-\u9fff]", model_output):
#         chinese_count += 1
#         print(model_output)
#
# # æ‰“å°ç»“æœ
# print(f"åŒ…å«ä¸­æ–‡çš„æ•°æ®æ¡æ•°: {chinese_count}")


import json
from rouge import Rouge
from sacrebleu.metrics import BLEU
import botok
import re
from botok import WordTokenizer
from botok.config import Config
from pathlib import Path
from tqdm import tqdm

# ğŸ”„ ä¿®æ”¹è¾“å‡ºæ–‡ä»¶è·¯å¾„
#output_results_file = "deepseek_outputs2.json"  # æ–°å¢è¾“å‡ºæ–‡ä»¶
eval_results_file = "LCTW-deepseek_results2.json"  # æ–°å¢è¯„ä¼°æ–‡ä»¶
#
# # âœ… ä½¿ç”¨botokè¿›è¡Œåˆ†è¯
# config = Config(dialect_name="general", base_path=Path.home())
# tokenizer = WordTokenizer(config=config)
#
#
# def segment_tibetan_text(text):
#     """ä½¿ç”¨ Botok è¿›è¡Œè—æ–‡åˆ†è¯"""
#     tokens = tokenizer.tokenize(text, split_affixes=False)
#     return " ".join([token.text for token in tokens])
#
#
# def tibetan_syllable_segment(text):
#     """ä½¿ç”¨è—è¯­éŸ³èŠ‚ç¬¦à¼‹è¿›è¡Œåˆ†è¯"""
#     # åœ¨æ¯ä¸ªéŸ³èŠ‚ç¬¦åæ·»åŠ ç©ºæ ¼ï¼Œåˆå¹¶å¤šä½™ç©ºæ ¼ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
#     segmented = text.replace('à¼‹', 'à¼‹ ')
#     segmented = re.sub(r' +', ' ', segmented)  # åˆå¹¶å¤šä¸ªè¿ç»­ç©ºæ ¼
#     return segmented.strip()
#
# # âœ… è®¡ç®— BLEU
# def compute_bleu1(pred: str, gold: str) -> float:
#     """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""
#
#     # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
#     bleu = BLEU(
#         smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
#         smooth_value=1,  # add-k çš„ k å€¼
#         max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
#         effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
#         tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
#         lowercase=False  # ä¸è½¬å°å†™
#     )
#
#     # âœ… è—æ–‡åˆ†è¯ï¼ˆå‡è®¾ tibetan_syllable_segment å·²æ­£ç¡®å®šä¹‰ï¼‰
#     pred_tok = segment_tibetan_text(pred)
#     gold_tok = segment_tibetan_text(gold)
#
#     # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
#     # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
#     bleu_score = bleu.sentence_score(pred_tok, [gold_tok])
#
#     return bleu_score.score
#
# # âœ… è®¡ç®— BLEU
# def compute_bleu2(pred: str, gold: str) -> float:
#     """åŸºäº SacreBLEU æºç çš„ BLEU è®¡ç®—å‡½æ•° (é€‚é…è—æ–‡åˆ†è¯)"""
#
#     # âœ… åˆå§‹åŒ– BLEU å‚æ•°ï¼ˆä¸æºç é»˜è®¤å‚æ•°å¯¹é½ï¼‰
#     bleu = BLEU(
#         smooth_method='add-k',  # å¹³æ»‘æ–¹æ³•
#         smooth_value=1,  # add-k çš„ k å€¼
#         max_ngram_order=4,  # æ˜¾å¼æŒ‡å®š BLEU-4
#         effective_order=True,  # ç¦ç”¨åŠ¨æ€ n-gram é˜¶æ•°
#         tokenize='none',  # ç¦ç”¨å†…ç½®åˆ†è¯ï¼ˆå·²æ‰‹åŠ¨åˆ†è¯ï¼‰
#         lowercase=False  # ä¸è½¬å°å†™
#     )
#
#     # âœ… è—æ–‡åˆ†è¯ï¼ˆå‡è®¾ tibetan_syllable_segment å·²æ­£ç¡®å®šä¹‰ï¼‰
#     pred_tok = tibetan_syllable_segment(pred)
#     gold_tok = tibetan_syllable_segment(gold)
#
#     # âœ… è°ƒç”¨ sentence_scoreï¼ˆå‚è€ƒ SacreBLEU æºç é€»è¾‘ï¼‰
#     # æ³¨æ„ï¼šreferences å¿…é¡»æ˜¯åˆ—è¡¨å½¢å¼ï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªå‚è€ƒ
#     bleu_score = bleu.sentence_score(pred_tok, [gold_tok])
#
#     return bleu_score.score
#
#
# # âœ… è®¡ç®— ROUGEï¼ˆä¿æŒä¸å˜ï¼‰
# rouge_evaluator = Rouge()
#
# def compute_rouge(pred, gold):
#     pred = __builtins__.str(pred).strip() or " "
#     gold = __builtins__.str(gold).strip() or " "
#
#     pred_clean = segment_tibetan_text(pred)
#     gold_clean = segment_tibetan_text(gold)
#
#     if not pred_clean.strip() or not gold_clean.strip():
#         return {"rouge-1": {"f": 0.0}, "rouge-2": {"f": 0.0}, "rouge-l": {"f": 0.0}}
#
#     try:
#         return rouge_evaluator.get_scores(pred_clean, gold_clean)[0]
#     except Exception as e:
#         print(f"âš ï¸ ROUGE è®¡ç®—å¤±è´¥: pred='{pred_clean}' gold='{gold_clean}'")
#         return {"rouge-1": {"f": 0.0}, "rouge-2": {"f": 0.0}, "rouge-l": {"f": 0.0}}
#
#
# # ğŸš€ åŠ è½½åŸå§‹ç»“æœï¼ˆä¸ä¿®æ”¹åŸæ–‡ä»¶ï¼‰
# with open("deepseek_outputs.json", "r", encoding="utf-8") as f:  # è¯»å–åŸå§‹æ–‡ä»¶
#     original_results = json.load(f)
#
# # åˆ›å»ºæ–°ç»“æœå‰¯æœ¬ï¼ˆé¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰
# new_results = [item.copy() for item in original_results]
#
# # ğŸ”„ é‡æ–°è®¡ç®—æŒ‡æ ‡
# print("ğŸ”„ å¼€å§‹é‡æ–°è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
# for item in tqdm(new_results, desc="ğŸ“Š è¯„ä¼°è¿›åº¦"):
#     if item.get("model_output") == "ERROR":
#         continue  # è·³è¿‡é”™è¯¯æ ·æœ¬
#
#     # é‡æ–°è®¡ç®—æŒ‡æ ‡
#     rouge_scores = compute_rouge(item["model_output"], item["gold_answer"])
#     bleu_scores1 = compute_bleu1(item["model_output"], item["gold_answer"])
#     bleu_scores2 = compute_bleu2(item["model_output"], item["gold_answer"])
#
#     # æ›´æ–°åˆ°æ–°ç»“æœ
#     item.update({
#         "rouge-1": rouge_scores["rouge-1"]["f"],
#         "rouge-2": rouge_scores["rouge-2"]["f"],
#         "rouge-l": rouge_scores["rouge-l"]["f"],
#         "bleu1": bleu_scores1,
#         "bleu2": bleu_scores2
#     })
#
# # ğŸ’¾ ä¿å­˜æ–°è¾“å‡ºæ–‡ä»¶
# with open(output_results_file, "w", encoding="utf-8") as f:
#     json.dump(new_results, f, ensure_ascii=False, indent=4)
# print(f"âœ… æ–°è¾“å‡ºå·²ä¿å­˜è‡³: {output_results_file}")

# ğŸš€ åŠ è½½åŸå§‹ç»“æœï¼ˆä¸ä¿®æ”¹åŸæ–‡ä»¶ï¼‰
with open("LCTW-deepseek_outputs.json", "r", encoding="utf-8") as f:  # è¯»å–åŸå§‹æ–‡ä»¶
    new_results = json.load(f)
# ğŸ“ˆ è®¡ç®—å¹³å‡æŒ‡æ ‡
valid_items = [item for item in new_results if item.get("model_output") != "ERROR"]
avg_scores = {
    "ROUGE-1": sum(i["rouge-1"] for i in valid_items) / len(valid_items),
    "ROUGE-2": sum(i["rouge-2"] for i in valid_items) / len(valid_items),
    "ROUGE-L": sum(i["rouge-l"] for i in valid_items) / len(valid_items),
    "bleu1": sum(i["bleu1"] for i in valid_items) / len(valid_items),
    "bleu2": sum(i["bleu2"] for i in valid_items) / len(valid_items)
}

# ğŸ’¾ ä¿å­˜æ–°è¯„ä¼°ç»“æœ
with open(eval_results_file, "w", encoding="utf-8") as f:
    json.dump(avg_scores, f, ensure_ascii=False, indent=4)
print(f"âœ… æ–°è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_results_file}")

# ğŸ“Š æ‰“å°ç»“æœ
print("\nğŸ“Š æœ€ç»ˆå¹³å‡æŒ‡æ ‡ï¼š")
print(json.dumps(avg_scores, indent=4, ensure_ascii=False))